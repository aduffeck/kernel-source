From: Linux Kernel Mailing List <linux-kernel@vger.kernel.org>
Subject: Linux: 3.12
Patch-mainline: 3.12

 This patch contains the differences between 3.11 and 3.12.

Automatically created from "patch-3.12" by xen-port-patches.py
Acked-by: jbeulich@suse.com

--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -1779,7 +1779,7 @@ config PHYSICAL_ALIGN
 	hex "Alignment value to which kernel should be aligned" if !XEN || EXPERT
 	default 0x2000 if XEN
 	default "0x200000"
-	range 0x2000 0x1000000 if X86_32
+	range 0x2000 0x1000000 if X86_32 || XEN
 	range 0x200000 0x1000000 if X86_64
 	---help---
 	  This value puts the alignment restrictions on physical address
@@ -2370,6 +2370,7 @@ source "drivers/rapidio/Kconfig"
 
 config X86_SYSFB
 	bool "Mark VGA/VBE/EFI FB as generic system framebuffer"
+	depends on !XEN_UNPRIVILEGED_GUEST
 	help
 	  Firmwares often provide initial graphics framebuffers so the BIOS,
 	  bootloader or kernel can show basic video-output during boot for
--- a/arch/x86/ia32/ia32entry-xen.S
+++ b/arch/x86/ia32/ia32entry-xen.S
@@ -342,7 +342,7 @@ ia32_badsys:
 
 	CFI_ENDPROC
 	
-	.macro PTREGSCALL label, func, arg
+	.macro PTREGSCALL label, func
 	ALIGN
 GLOBAL(\label)
 	leaq \func(%rip),%rax
--- a/arch/x86/include/mach-xen/asm/mmu_context.h
+++ b/arch/x86/include/mach-xen/asm/mmu_context.h
@@ -105,9 +105,7 @@ static inline void switch_mm(struct mm_s
 		op++;
 #endif
 
-		/*
-		 * load the LDT, if the LDT is different:
-		 */
+		/* Load the LDT, if the LDT is different: */
 		if (unlikely(prev->context.ldt != next->context.ldt)) {
 			/* load_LDT_nolock(&next->context) */
 			op->cmd = MMUEXT_SET_LDT;
@@ -118,7 +116,7 @@ static inline void switch_mm(struct mm_s
 
 		BUG_ON(HYPERVISOR_mmuext_op(_op, op-_op, NULL, DOMID_SELF));
 
-		/* stop TLB flushes for the previous mm */
+		/* Stop TLB flushes for the previous mm */
 		cpumask_clear_cpu(cpu, mm_cpumask(prev));
 	}
 #if defined(CONFIG_SMP) && !defined(CONFIG_XEN) /* XEN: no lazy tlb */
@@ -126,8 +124,16 @@ static inline void switch_mm(struct mm_s
 		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);
 		BUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);
 
-		if (!cpumask_test_and_set_cpu(cpu, mm_cpumask(next))) {
-			/* We were in lazy tlb mode and leave_mm disabled
+		if (!cpumask_test_cpu(cpu, mm_cpumask(next))) {
+			/*
+			 * On established mms, the mm_cpumask is only changed
+			 * from irq context, from ptep_clear_flush() while in
+			 * lazy tlb mode, and here. Irqs are blocked during
+			 * schedule, protecting us from simultaneous changes.
+			 */
+			cpumask_set_cpu(cpu, mm_cpumask(next));
+			/*
+			 * We were in lazy tlb mode and leave_mm disabled
 			 * tlb flush IPI delivery. We must reload CR3
 			 * to make sure to use no freed page tables.
 			 */
--- a/arch/x86/include/mach-xen/asm/pci.h
+++ b/arch/x86/include/mach-xen/asm/pci.h
@@ -106,29 +106,6 @@ static inline void early_quirks(void) {
 extern void pci_iommu_alloc(void);
 
 #if defined(CONFIG_PCI_MSI) && !defined(CONFIG_XEN)
-/* MSI arch specific hooks */
-static inline int x86_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)
-{
-	return x86_msi.setup_msi_irqs(dev, nvec, type);
-}
-
-static inline void x86_teardown_msi_irqs(struct pci_dev *dev)
-{
-	x86_msi.teardown_msi_irqs(dev);
-}
-
-static inline void x86_teardown_msi_irq(unsigned int irq)
-{
-	x86_msi.teardown_msi_irq(irq);
-}
-static inline void x86_restore_msi_irqs(struct pci_dev *dev, int irq)
-{
-	x86_msi.restore_msi_irqs(dev, irq);
-}
-#define arch_setup_msi_irqs x86_setup_msi_irqs
-#define arch_teardown_msi_irqs x86_teardown_msi_irqs
-#define arch_teardown_msi_irq x86_teardown_msi_irq
-#define arch_restore_msi_irqs x86_restore_msi_irqs
 /* implemented in arch/x86/kernel/apic/io_apic. */
 struct msi_desc;
 int native_setup_msi_irqs(struct pci_dev *dev, int nvec, int type);
@@ -136,16 +113,9 @@ void native_teardown_msi_irq(unsigned in
 void native_restore_msi_irqs(struct pci_dev *dev, int irq);
 int setup_msi_irq(struct pci_dev *dev, struct msi_desc *msidesc,
 		  unsigned int irq_base, unsigned int irq_offset);
-/* default to the implementation in drivers/lib/msi.c */
-#define HAVE_DEFAULT_MSI_TEARDOWN_IRQS
-#define HAVE_DEFAULT_MSI_RESTORE_IRQS
-void default_teardown_msi_irqs(struct pci_dev *dev);
-void default_restore_msi_irqs(struct pci_dev *dev, int irq);
 #else
 #define native_setup_msi_irqs		NULL
 #define native_teardown_msi_irq		NULL
-#define default_teardown_msi_irqs	NULL
-#define default_restore_msi_irqs	NULL
 #endif
 
 #define PCI_DMA_BUS_IS_PHYS 0
--- a/arch/x86/include/mach-xen/asm/pgtable.h
+++ b/arch/x86/include/mach-xen/asm/pgtable.h
@@ -22,7 +22,8 @@
  * ZERO_PAGE is a global shared page that is always zero: used
  * for zero-mapped memory areas etc..
  */
-extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
+extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)]
+	__visible;
 #define ZERO_PAGE(vaddr) (virt_to_page(empty_zero_page))
 
 extern spinlock_t pgd_lock;
@@ -313,21 +314,6 @@ static inline pmd_t pmd_mksoft_dirty(pmd
 }
 #endif
 
-static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
-{
-	return pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);
-}
-
-static inline int pte_swp_soft_dirty(pte_t pte)
-{
-	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;
-}
-
-static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
-{
-	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);
-}
-
 static inline pte_t pte_file_clear_soft_dirty(pte_t pte)
 {
 	return pte_clear_flags(pte, _PAGE_SOFT_DIRTY);
@@ -447,6 +433,7 @@ pte_t *populate_extra_pte(unsigned long
 
 #ifndef __ASSEMBLY__
 #include <linux/mm_types.h>
+#include <linux/mmdebug.h>
 #include <linux/log2.h>
 
 static inline int pte_none(pte_t pte)
@@ -963,6 +950,26 @@ static inline void ptep_modify_prot_comm
 		BUG();
 }
 
+#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY
+static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
+{
+	VM_BUG_ON(pte_present(pte));
+	return pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);
+}
+
+static inline int pte_swp_soft_dirty(pte_t pte)
+{
+	VM_BUG_ON(pte_present(pte));
+	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;
+}
+
+static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
+{
+	VM_BUG_ON(pte_present(pte));
+	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);
+}
+#endif
+
 #include <asm-generic/pgtable.h>
 
 #include <xen/features.h>
--- a/arch/x86/include/mach-xen/asm/pgtable_types.h
+++ b/arch/x86/include/mach-xen/asm/pgtable_types.h
@@ -75,6 +75,9 @@
  * with swap entry format. On x86 bits 6 and 7 are *not* involved
  * into swap entry computation, but bit 6 is used for nonlinear
  * file mapping, so we borrow bit 7 for soft dirty tracking.
+ *
+ * Please note that this bit must be treated as swap dirty page
+ * mark if and only if the PTE has present bit clear!
  */
 #ifdef CONFIG_MEM_SOFT_DIRTY
 #define _PAGE_SWP_SOFT_DIRTY	_PAGE_PSE
--- a/arch/x86/include/mach-xen/asm/processor.h
+++ b/arch/x86/include/mach-xen/asm/processor.h
@@ -430,7 +430,7 @@ union irq_stack_union {
 	};
 };
 
-DECLARE_PER_CPU_FIRST(union irq_stack_union, irq_stack_union);
+DECLARE_PER_CPU_FIRST(union irq_stack_union, irq_stack_union) __visible;
 DECLARE_INIT_PER_CPU(irq_stack_union);
 
 DECLARE_PER_CPU(char *, irq_stack_ptr);
@@ -939,36 +939,20 @@ extern int set_tsc_mode(unsigned int val
 
 extern u16 amd_get_nb_id(int cpu);
 
-#ifndef CONFIG_XEN
-struct aperfmperf {
-	u64 aperf, mperf;
-};
-
-static inline void get_aperfmperf(struct aperfmperf *am)
+static inline uint32_t hypervisor_cpuid_base(const char *sig, uint32_t leaves)
 {
-	WARN_ON_ONCE(!boot_cpu_has(X86_FEATURE_APERFMPERF));
-
-	rdmsrl(MSR_IA32_APERF, am->aperf);
-	rdmsrl(MSR_IA32_MPERF, am->mperf);
-}
+	uint32_t base, eax, signature[3];
 
-#define APERFMPERF_SHIFT 10
+	for (base = 0x40000000; base < 0x40010000; base += 0x100) {
+		cpuid(base, &eax, &signature[0], &signature[1], &signature[2]);
 
-static inline
-unsigned long calc_aperfmperf_ratio(struct aperfmperf *old,
-				    struct aperfmperf *new)
-{
-	u64 aperf = new->aperf - old->aperf;
-	u64 mperf = new->mperf - old->mperf;
-	unsigned long ratio = aperf;
-
-	mperf >>= APERFMPERF_SHIFT;
-	if (mperf)
-		ratio = div64_u64(aperf, mperf);
+		if (!memcmp(sig, signature, 12) &&
+		    (leaves == 0 || ((eax - base) >= leaves)))
+			return base;
+	}
 
-	return ratio;
+	return 0;
 }
-#endif
 
 extern unsigned long arch_align_stack(unsigned long sp);
 extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
--- a/arch/x86/include/mach-xen/asm/spinlock.h
+++ b/arch/x86/include/mach-xen/asm/spinlock.h
@@ -40,6 +40,9 @@
 
 #ifdef TICKET_SHIFT
 
+/* How long a lock should spin before we consider blocking */
+#define SPIN_THRESHOLD	(1 << 15)
+
 #include <asm/irqflags.h>
 
 int xen_spinlock_init(unsigned int cpu);
@@ -55,6 +58,11 @@ bool xen_spin_wait(arch_spinlock_t *, st
 		   unsigned int flags);
 void xen_spin_kick(const arch_spinlock_t *, unsigned int ticket);
 
+static __always_inline int __ticket_spin_value_unlocked(arch_spinlock_t lock)
+{
+	return lock.tickets.head == lock.tickets.tail;
+}
+
 /*
  * Ticket locks are conceptually two parts, one indicating the current head of
  * the queue, and the other indicating the current tail. The lock is acquired
@@ -71,7 +79,7 @@ void xen_spin_kick(const arch_spinlock_t
 #if CONFIG_XEN_SPINLOCK_ACQUIRE_NESTING
 static __always_inline void __ticket_spin_lock(arch_spinlock_t *lock)
 {
-	struct __raw_tickets inc = { .tail = 1 };
+	struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };
 	unsigned int count, flags = arch_local_irq_save();
 
 	inc = xadd(&lock->tickets, inc);
@@ -83,7 +91,7 @@ static __always_inline void __ticket_spi
 	arch_local_irq_restore(flags);
 
 	do {
-		count = 1 << 12;
+		count = SPIN_THRESHOLD;
 		while (inc.head != inc.tail && --count) {
 			cpu_relax();
 			inc.head = ACCESS_ONCE(lock->tickets.head);
@@ -98,7 +106,7 @@ static __always_inline void __ticket_spi
 static __always_inline void __ticket_spin_lock_flags(arch_spinlock_t *lock,
 						     unsigned long flags)
 {
-	struct __raw_tickets inc = { .tail = 1 };
+	struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };
 	unsigned int count;
 
 	inc = xadd(&lock->tickets, inc);
@@ -107,7 +115,7 @@ static __always_inline void __ticket_spi
 	inc = xen_spin_adjust(lock, inc);
 
 	do {
-		count = 1 << 12;
+		count = SPIN_THRESHOLD;
 		while (inc.head != inc.tail && --count) {
 			cpu_relax();
 			inc.head = ACCESS_ONCE(lock->tickets.head);
@@ -124,7 +132,7 @@ static __always_inline int __ticket_spin
 	if (old.tickets.head != old.tickets.tail)
 		return 0;
 
-	new.head_tail = old.head_tail + (1 << TICKET_SHIFT);
+	new.head_tail = old.head_tail + (TICKET_LOCK_INC << TICKET_SHIFT);
 
 	/* cmpxchg is a full barrier, so nothing can move before it */
 	return cmpxchg(&lock->head_tail, old.head_tail, new.head_tail) == old.head_tail;
@@ -164,6 +172,11 @@ static inline int __ticket_spin_is_conte
 static inline int xen_spinlock_init(unsigned int cpu) { return 0; }
 static inline void xen_spinlock_cleanup(unsigned int cpu) {}
 
+static __always_inline int __byte_spin_value_unlocked(arch_spinlock_t lock)
+{
+	return lock.lock == 0;
+}
+
 static inline int __byte_spin_is_locked(arch_spinlock_t *lock)
 {
 	return lock->lock != 0;
@@ -213,6 +226,11 @@ static inline void __byte_spin_unlock(ar
 
 #endif /* TICKET_SHIFT */
 
+static __always_inline int arch_spin_value_unlocked(arch_spinlock_t lock)
+{
+	return __arch_spin(value_unlocked)(lock);
+}
+
 static inline int arch_spin_is_locked(arch_spinlock_t *lock)
 {
 	return __arch_spin(is_locked)(lock);
--- a/arch/x86/include/mach-xen/asm/spinlock_types.h
+++ b/arch/x86/include/mach-xen/asm/spinlock_types.h
@@ -1,13 +1,11 @@
 #ifndef _ASM_X86_SPINLOCK_TYPES_H
 #define _ASM_X86_SPINLOCK_TYPES_H
 
-#ifndef __LINUX_SPINLOCK_TYPES_H
-# error "please don't include this file directly"
-#endif
-
 #include <linux/types.h>
 
 #ifdef CONFIG_XEN_SPINLOCK_ACQUIRE_NESTING
+#define __TICKET_LOCK_INC	1
+#define TICKET_SLOWPATH_FLAG	((__ticket_t)0)
 /*
  * On Xen we support CONFIG_XEN_SPINLOCK_ACQUIRE_NESTING levels of
  * interrupt re-enabling per IRQ-safe lock. Hence we can have
@@ -25,6 +23,8 @@ typedef u16 __ticket_t;
 typedef u32 __ticketpair_t;
 #endif
 
+#define TICKET_LOCK_INC	((__ticket_t)__TICKET_LOCK_INC)
+
 typedef union {
 	__ticketpair_t head_tail;
 	struct __raw_tickets {
--- a/arch/x86/include/mach-xen/asm/tlbflush.h
+++ b/arch/x86/include/mach-xen/asm/tlbflush.h
@@ -3,6 +3,7 @@
 
 #include <linux/mm.h>
 #include <linux/sched.h>
+#include <linux/vmstat.h>
 
 #include <asm/processor.h>
 #include <asm/special_insns.h>
@@ -18,6 +19,7 @@ static inline void __flush_tlb_all(void)
 
 static inline void __flush_tlb_one(unsigned long addr)
 {
+	count_vm_event(NR_TLB_LOCAL_FLUSH_ONE);
 	__flush_tlb_single(addr);
 }
 
@@ -39,14 +41,38 @@ static inline void __flush_tlb_one(unsig
 
 #ifndef CONFIG_SMP
 
-#define flush_tlb() __flush_tlb()
-#define flush_tlb_all() __flush_tlb_all()
-#define local_flush_tlb() __flush_tlb()
+/* "_up" is for UniProcessor.
+ *
+ * This is a helper for other header functions.  *Not* intended to be called
+ * directly.  All global TLB flushes need to either call this, or to bump the
+ * vm statistics themselves.
+ */
+static inline void __flush_tlb_up(void)
+{
+	count_vm_event(NR_TLB_LOCAL_FLUSH_ALL);
+	__flush_tlb();
+}
+
+static inline void flush_tlb_all(void)
+{
+	count_vm_event(NR_TLB_LOCAL_FLUSH_ALL);
+	__flush_tlb_all();
+}
+
+static inline void flush_tlb(void)
+{
+	__flush_tlb_up();
+}
+
+static inline void local_flush_tlb(void)
+{
+	__flush_tlb_up();
+}
 
 static inline void flush_tlb_mm(struct mm_struct *mm)
 {
 	if (mm == current->active_mm)
-		__flush_tlb();
+		__flush_tlb_up();
 }
 
 static inline void flush_tlb_page(struct vm_area_struct *vma,
@@ -60,14 +86,14 @@ static inline void flush_tlb_range(struc
 				   unsigned long start, unsigned long end)
 {
 	if (vma->vm_mm == current->active_mm)
-		__flush_tlb();
+		__flush_tlb_up();
 }
 
 static inline void flush_tlb_mm_range(struct mm_struct *mm,
 	   unsigned long start, unsigned long end, unsigned long vmflag)
 {
 	if (mm == current->active_mm)
-		__flush_tlb();
+		__flush_tlb_up();
 }
 
 static inline void reset_lazy_tlbstate(void)
--- a/arch/x86/include/mach-xen/asm/xenoprof.h
+++ b/arch/x86/include/mach-xen/asm/xenoprof.h
@@ -25,7 +25,7 @@
 
 struct super_block;
 struct dentry;
-int xenoprof_create_files(struct super_block * sb, struct dentry * root);
+int xenoprof_create_files(struct dentry *);
 #define HAVE_XENOPROF_CREATE_FILES
 
 struct xenoprof_init;
--- a/arch/x86/kernel/acpi/processor_extcntl_xen.c
+++ b/arch/x86/kernel/acpi/processor_extcntl_xen.c
@@ -190,7 +190,7 @@ static struct processor_extcntl_ops xen_
 	.hotplug		= xen_hotplug_notifier,
 };
 
-static int xen_sleep(u8 sleep_state, u32 val_a, u32 val_b, bool extended)
+static int _xen_sleep(u8 sleep_state, u32 val_a, u32 val_b, unsigned int flags)
 {
 	struct xen_platform_op op = {
 		.cmd = XENPF_enter_acpi_sleep,
@@ -199,7 +199,7 @@ static int xen_sleep(u8 sleep_state, u32
 			.val_a = val_a,
 			.val_b = val_b,
 			.sleep_state = sleep_state,
-			.flags = extended ? XENPF_ACPI_SLEEP_EXTENDED : 0,
+			.flags = flags,
 		},
 	};
 	int err = HYPERVISOR_platform_op(&op);
@@ -211,6 +211,16 @@ static int xen_sleep(u8 sleep_state, u32
 	return -1;
 }
 
+static int xen_sleep(u8 sleep_state, u32 pm1a_cnt_val, u32 pm1b_cnt_val)
+{
+	return _xen_sleep(sleep_state, pm1a_cnt_val, pm1b_cnt_val, 0);
+}
+
+static int xen_extended_sleep(u8 sleep_state, u32 val_a, u32 val_b)
+{
+	return _xen_sleep(sleep_state, val_a, val_b, XENPF_ACPI_SLEEP_EXTENDED);
+}
+
 static int xen_acpi_suspend_lowlevel(void)
 {
 	acpi_enter_sleep_state(ACPI_STATE_S3);
@@ -222,6 +232,7 @@ static int __init init_extcntl(void)
 	unsigned int pmbits = (xen_start_info->flags & SIF_PM_MASK) >> 8;
 
 	acpi_os_set_prepare_sleep(xen_sleep);
+	acpi_os_set_prepare_extended_sleep(xen_extended_sleep);
 	acpi_suspend_lowlevel = xen_acpi_suspend_lowlevel;
 
 	if (!pmbits)
--- a/arch/x86/kernel/apic/io_apic-xen.c
+++ b/arch/x86/kernel/apic/io_apic-xen.c
@@ -394,7 +394,6 @@ union entry_union {
 	struct IO_APIC_route_entry entry;
 };
 
-#ifndef CONFIG_XEN
 static struct IO_APIC_route_entry __ioapic_read_entry(int apic, int pin)
 {
 	union entry_union eu;
@@ -416,7 +415,6 @@ static struct IO_APIC_route_entry ioapic
 
 	return eu.entry;
 }
-#endif
 
 /*
  * When we write a new IO APIC routing entry, we need to write the high
@@ -1617,6 +1615,11 @@ void intel_ir_io_apic_print_entries(unsi
 	}
 }
 
+void ioapic_zap_locks(void)
+{
+	raw_spin_lock_init(&ioapic_lock);
+}
+
 __apicdebuginit(void) print_IO_APIC(int ioapic_idx)
 {
 	union IO_APIC_reg_00 reg_00;
@@ -3479,12 +3482,15 @@ int io_apic_setup_irq_pin_once(unsigned
 {
 	unsigned int ioapic_idx = attr->ioapic, pin = attr->ioapic_pin;
 	int ret;
+	struct IO_APIC_route_entry orig_entry;
 
 	/* Avoid redundant programming */
 	if (test_bit(pin, ioapics[ioapic_idx].pin_programmed)) {
-		pr_debug("Pin %d-%d already programmed\n",
-			 mpc_ioapic_id(ioapic_idx), pin);
-		return 0;
+		pr_debug("Pin %d-%d already programmed\n", mpc_ioapic_id(ioapic_idx), pin);
+		orig_entry = ioapic_read_entry(attr->ioapic, pin);
+		if (attr->trigger == orig_entry.trigger && attr->polarity == orig_entry.polarity)
+			return 0;
+		return -EBUSY;
 	}
 	ret = io_apic_setup_irq_pin(irq, node, attr);
 	if (!ret)
--- a/arch/x86/kernel/cpu/common-xen.c
+++ b/arch/x86/kernel/cpu/common-xen.c
@@ -1137,7 +1137,7 @@ struct desc_ptr debug_idt_descr = { NR_V
 #endif
 
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
-		     irq_stack_union) __aligned(PAGE_SIZE);
+		     irq_stack_union) __aligned(PAGE_SIZE) __visible;
 
 void xen_switch_pt(void)
 {
@@ -1162,7 +1162,7 @@ EXPORT_PER_CPU_SYMBOL(kernel_stack);
 DEFINE_PER_CPU(char *, irq_stack_ptr) =
 	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;
 
-DEFINE_PER_CPU(unsigned int, irq_count) = -1;
+DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;
 
 DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
 
--- a/arch/x86/kernel/e820-xen.c
+++ b/arch/x86/kernel/e820-xen.c
@@ -713,15 +713,18 @@ __init void e820_setup_gap(void)
  * boot_params.e820_map, others are passed via SETUP_E820_EXT node of
  * linked list of struct setup_data, which is parsed here.
  */
-void __init parse_e820_ext(struct setup_data *sdata)
+void __init parse_e820_ext(u64 phys_addr, u32 data_len)
 {
 	int entries;
 	struct e820entry *extmap;
+	struct setup_data *sdata;
 
+	sdata = early_memremap(phys_addr, data_len);
 	entries = sdata->len / sizeof(struct e820entry);
 	extmap = (struct e820entry *)(sdata->data);
 	__append_e820_map(extmap, entries);
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
+	early_iounmap(sdata, data_len);
 	printk(KERN_INFO "e820: extended physical RAM map:\n");
 	_e820_print_map(&e820, "extended");
 }
--- a/arch/x86/kernel/entry_32-xen.S
+++ b/arch/x86/kernel/entry_32-xen.S
@@ -1296,6 +1296,9 @@ ftrace_restore_flags:
 #else /* ! CONFIG_DYNAMIC_FTRACE */
 
 ENTRY(mcount)
+	cmpl $__PAGE_OFFSET, %esp
+	jb ftrace_stub		/* Paging not enabled yet? */
+
 	cmpl $0, function_trace_stop
 	jne  ftrace_stub
 
--- a/arch/x86/kernel/entry_64-xen.S
+++ b/arch/x86/kernel/entry_64-xen.S
@@ -520,24 +520,7 @@ NMI_MASK = 0x80000000
 	/* We entered an interrupt context - irqs are off: */
 	TRACE_IRQS_OFF
 	.endm
-#endif
 
-ENTRY(save_rest)
-	PARTIAL_FRAME 1 (REST_SKIP+8)
-	movq 5*8+16(%rsp), %r11	/* save return address */
-	movq_cfi rbx, RBX+16
-	movq_cfi rbp, RBP+16
-	movq_cfi r12, R12+16
-	movq_cfi r13, R13+16
-	movq_cfi r14, R14+16
-	movq_cfi r15, R15+16
-	movq %r11, 8(%rsp)	/* return address */
-	FIXUP_TOP_OF_STACK %r11, 16
-	ret
-	CFI_ENDPROC
-END(save_rest)
-
-#ifndef CONFIG_XEN
 /* save complete stack frame */
 	.pushsection .kprobes.text, "ax"
 ENTRY(save_paranoid)
--- a/arch/x86/kernel/head32-xen.c
+++ b/arch/x86/kernel/head32-xen.c
@@ -28,7 +28,7 @@ static void __init i386_default_early_se
 #endif
 }
 
-void __init i386_start_kernel(void)
+asmlinkage void __init i386_start_kernel(void)
 {
 #ifdef CONFIG_XEN
 	struct xen_platform_parameters pp;
--- a/arch/x86/kernel/head64-xen.c
+++ b/arch/x86/kernel/head64-xen.c
@@ -153,7 +153,7 @@ static void __init copy_bootdata(char *r
 
 #include <xen/interface/memory.h>
 
-void __init x86_64_start_kernel(char * real_mode_data)
+asmlinkage void __init x86_64_start_kernel(char * real_mode_data)
 {
 	/*
 	 * Build-time sanity checks on the kernel image and module
--- a/arch/x86/kernel/irq-xen.c
+++ b/arch/x86/kernel/irq-xen.c
@@ -196,7 +196,7 @@ u64 arch_irq_stat(void)
  * SMP cross-CPU interrupts have their own specific
  * handlers).
  */
-unsigned int __irq_entry do_IRQ(struct pt_regs *regs)
+__visible unsigned int __irq_entry do_IRQ(struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
 
@@ -234,7 +234,7 @@ void __smp_x86_platform_ipi(void)
 		x86_platform_ipi_callback();
 }
 
-void smp_x86_platform_ipi(struct pt_regs *regs)
+__visible void smp_x86_platform_ipi(struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
 
@@ -248,7 +248,7 @@ void smp_x86_platform_ipi(struct pt_regs
 /*
  * Handler for POSTED_INTERRUPT_VECTOR.
  */
-void smp_kvm_posted_intr_ipi(struct pt_regs *regs)
+__visible void smp_kvm_posted_intr_ipi(struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
 
@@ -266,7 +266,7 @@ void smp_kvm_posted_intr_ipi(struct pt_r
 }
 #endif
 
-void smp_trace_x86_platform_ipi(struct pt_regs *regs)
+__visible void smp_trace_x86_platform_ipi(struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
 
--- a/arch/x86/kernel/process-xen.c
+++ b/arch/x86/kernel/process-xen.c
@@ -38,7 +38,7 @@
  * section. Since TSS's are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
-DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
+__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
 #endif
 
 #ifdef CONFIG_X86_64
--- a/arch/x86/kernel/process_32-xen.c
+++ b/arch/x86/kernel/process_32-xen.c
@@ -255,7 +255,7 @@ EXPORT_SYMBOL_GPL(start_thread);
  * the task-switch, and shows up in ret_from_fork in entry.S,
  * for example.
  */
-__notrace_funcgraph struct task_struct *
+__visible __notrace_funcgraph struct task_struct *
 __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	struct thread_struct *prev = &prev_p->thread,
--- a/arch/x86/kernel/process_64-xen.c
+++ b/arch/x86/kernel/process_64-xen.c
@@ -282,7 +282,7 @@ void start_thread_ia32(struct pt_regs *r
  * Kprobes not supported here. Set the probe on schedule instead.
  * Function graph tracer not supported too.
  */
-__notrace_funcgraph struct task_struct *
+__visible __notrace_funcgraph struct task_struct *
 __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	struct thread_struct *prev = &prev_p->thread;
--- a/arch/x86/kernel/setup-xen.c
+++ b/arch/x86/kernel/setup-xen.c
@@ -239,9 +239,9 @@ EXPORT_SYMBOL(boot_cpu_data);
 
 
 #if !defined(CONFIG_X86_PAE) || defined(CONFIG_X86_64)
-unsigned long mmu_cr4_features;
+__visible unsigned long mmu_cr4_features;
 #else
-unsigned long mmu_cr4_features = X86_CR4_PAE;
+__visible unsigned long mmu_cr4_features = X86_CR4_PAE;
 #endif
 
 /* Boot loader ID and version as integers, for the benefit of proc_dointvec */
@@ -489,25 +489,23 @@ static void __init parse_setup_data(void
 {
 #ifndef CONFIG_XEN
 	struct setup_data *data;
-	u64 pa_data;
+	u64 pa_data, pa_next;
 
 	pa_data = boot_params.hdr.setup_data;
 	while (pa_data) {
-		u32 data_len, map_len;
+		u32 data_len, map_len, data_type;
 
 		map_len = max(PAGE_SIZE - (pa_data & ~PAGE_MASK),
 			      (u64)sizeof(struct setup_data));
 		data = early_memremap(pa_data, map_len);
 		data_len = data->len + sizeof(struct setup_data);
-		if (data_len > map_len) {
-			early_iounmap(data, map_len);
-			data = early_memremap(pa_data, data_len);
-			map_len = data_len;
-		}
+		data_type = data->type;
+		pa_next = data->next;
+		early_iounmap(data, map_len);
 
-		switch (data->type) {
+		switch (data_type) {
 		case SETUP_E820_EXT:
-			parse_e820_ext(data);
+			parse_e820_ext(pa_data, data_len);
 			break;
 		case SETUP_DTB:
 			add_dtb(pa_data);
@@ -515,8 +513,7 @@ static void __init parse_setup_data(void
 		default:
 			break;
 		}
-		pa_data = data->next;
-		early_iounmap(data, map_len);
+		pa_data = pa_next;
 	}
 #endif
 }
@@ -1213,7 +1210,7 @@ void __init setup_arch(char **cmdline_p)
 
 	cleanup_highmap();
 
-	memblock.current_limit = ISA_END_ADDRESS;
+	memblock_set_current_limit(ISA_END_ADDRESS);
 	memblock_x86_fill();
 
 	/*
@@ -1250,7 +1247,7 @@ void __init setup_arch(char **cmdline_p)
 	setup_real_mode();
 #endif
 
-	memblock.current_limit = get_max_mapped();
+	memblock_set_current_limit(get_max_mapped());
 	dma_contiguous_reserve(0);
 
 	/*
--- a/arch/x86/kernel/syscall_32-xen.c
+++ b/arch/x86/kernel/syscall_32-xen.c
@@ -9,7 +9,7 @@ extern asmlinkage void cstar_set_tif(voi
 #define	ptregs_clone cstar_set_tif
 #define	ptregs_vfork cstar_set_tif
 
-const sys_call_ptr_t cstar_call_table[__NR_syscall_max+1] = {
+__visible const sys_call_ptr_t cstar_call_table[__NR_syscall_max+1] = {
 	/*
 	 * Smells like a compiler bug -- it doesn't work
 	 * when the & below is removed.
--- a/arch/x86/kernel/traps-xen.c
+++ b/arch/x86/kernel/traps-xen.c
@@ -58,6 +58,7 @@
 #include <asm/mce.h>
 #include <asm/fixmap.h>
 #include <asm/mach_traps.h>
+#include <asm/alternative.h>
 
 #ifdef CONFIG_X86_64
 #include <asm/x86_init.h>
@@ -333,6 +334,9 @@ dotraplinkage void __kprobes notrace do_
 	    ftrace_int3_handler(regs))
 		return;
 #endif
+	if (poke_int3_handler(regs))
+		return;
+
 	prev_state = exception_enter();
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
--- a/arch/x86/mm/fault-xen.c
+++ b/arch/x86/mm/fault-xen.c
@@ -851,23 +851,15 @@ do_sigbus(struct pt_regs *regs, unsigned
 	force_sig_info_fault(SIGBUS, code, address, tsk, fault);
 }
 
-static noinline int
+static noinline void
 mm_fault_error(struct pt_regs *regs, unsigned long error_code,
 	       unsigned long address, unsigned int fault)
 {
-	/*
-	 * Pagefault was interrupted by SIGKILL. We have no reason to
-	 * continue pagefault.
-	 */
-	if (fatal_signal_pending(current)) {
-		if (!(fault & VM_FAULT_RETRY))
-			up_read(&current->mm->mmap_sem);
-		if (!(error_code & PF_USER))
-			no_context(regs, error_code, address, 0, 0);
-		return 1;
+	if (fatal_signal_pending(current) && !(error_code & PF_USER)) {
+		up_read(&current->mm->mmap_sem);
+		no_context(regs, error_code, address, 0, 0);
+		return;
 	}
-	if (!(fault & VM_FAULT_ERROR))
-		return 0;
 
 	if (fault & VM_FAULT_OOM) {
 		/* Kernel mode? Handle exceptions or die: */
@@ -875,7 +867,7 @@ mm_fault_error(struct pt_regs *regs, uns
 			up_read(&current->mm->mmap_sem);
 			no_context(regs, error_code, address,
 				   SIGSEGV, SEGV_MAPERR);
-			return 1;
+			return;
 		}
 
 		up_read(&current->mm->mmap_sem);
@@ -893,7 +885,6 @@ mm_fault_error(struct pt_regs *regs, uns
 		else
 			BUG();
 	}
-	return 1;
 }
 
 static int spurious_fault_check(unsigned long error_code, pte_t *pte)
@@ -1020,9 +1011,7 @@ __do_page_fault(struct pt_regs *regs, un
 	unsigned long address;
 	struct mm_struct *mm;
 	int fault;
-	int write = error_code & PF_WRITE;
-	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE |
-					(write ? FAULT_FLAG_WRITE : 0);
+	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
 	/* Set the "privileged fault" bit to something sane. */
 	if (user_mode_vm(regs))
@@ -1112,6 +1101,7 @@ __do_page_fault(struct pt_regs *regs, un
 	if (user_mode_vm(regs)) {
 		local_irq_enable();
 		error_code |= PF_USER;
+		flags |= FAULT_FLAG_USER;
 	} else {
 		if (regs->flags & X86_EFLAGS_IF)
 			local_irq_enable();
@@ -1138,6 +1128,9 @@ __do_page_fault(struct pt_regs *regs, un
 		return;
 	}
 
+	if (error_code & PF_WRITE)
+		flags |= FAULT_FLAG_WRITE;
+
 	/*
 	 * When running in the kernel we expect faults to occur only to
 	 * addresses in user space.  All other faults represent errors in
@@ -1216,9 +1209,17 @@ good_area:
 	 */
 	fault = handle_mm_fault(mm, vma, address, flags);
 
-	if (unlikely(fault & (VM_FAULT_RETRY|VM_FAULT_ERROR))) {
-		if (mm_fault_error(regs, error_code, address, fault))
-			return;
+	/*
+	 * If we need to retry but a fatal signal is pending, handle the
+	 * signal first. We do not need to release the mmap_sem because it
+	 * would already be released in __lock_page_or_retry in mm/filemap.c.
+	 */
+	if (unlikely((fault & VM_FAULT_RETRY) && fatal_signal_pending(current)))
+		return;
+
+	if (unlikely(fault & VM_FAULT_ERROR)) {
+		mm_fault_error(regs, error_code, address, fault);
+		return;
 	}
 
 	/*
--- a/arch/x86/mm/ioremap-xen.c
+++ b/arch/x86/mm/ioremap-xen.c
@@ -664,7 +664,7 @@ __early_ioremap(resource_size_t phys_add
 	unsigned long offset;
 	resource_size_t last_addr;
 	unsigned int nrpages;
-	enum fixed_addresses idx0, idx;
+	enum fixed_addresses idx;
 	int i, slot;
 
 	WARN_ON(system_state != SYSTEM_BOOTING);
@@ -717,8 +717,7 @@ __early_ioremap(resource_size_t phys_add
 	/*
 	 * Ok, go for it..
 	 */
-	idx0 = FIX_BTMAP_BEGIN - NR_FIX_BTMAPS*slot;
-	idx = idx0;
+	idx = FIX_BTMAP_BEGIN - NR_FIX_BTMAPS*slot;
 	while (nrpages > 0) {
 		early_set_fixmap(idx, phys_addr, prot);
 		phys_addr += PAGE_SIZE;
--- a/arch/x86/mm/tlb-xen.c
+++ b/arch/x86/mm/tlb-xen.c
@@ -73,10 +73,13 @@ void flush_tlb_mm_range(struct mm_struct
 	    <= (act_entries >> tlb_flushall_shift)
 	    && !has_large_page(mm, start, end)) {
 		/* flush range by one by one 'invlpg' */
-		for (addr = start; addr < end; addr += PAGE_SIZE)
+		for (addr = start; addr < end; addr += PAGE_SIZE) {
+			count_vm_event(NR_TLB_LOCAL_FLUSH_ONE);
 			xen_invlpg_mask(mask, addr);
+		}
 	} else {
 flush_all:
+		count_vm_event(NR_TLB_LOCAL_FLUSH_ALL);
 		xen_tlb_flush_mask(mask);
 	}
 
--- a/arch/x86/oprofile/xenoprof.c
+++ b/arch/x86/oprofile/xenoprof.c
@@ -141,7 +141,7 @@ out:
 
 struct op_counter_config counter_config[OP_MAX_COUNTER];
 
-int xenoprof_create_files(struct super_block * sb, struct dentry * root)
+int xenoprof_create_files(struct dentry *root)
 {
 	unsigned int i;
 
@@ -150,20 +150,20 @@ int xenoprof_create_files(struct super_b
 		char buf[4];
  
 		snprintf(buf, sizeof(buf), "%d", i);
-		dir = oprofilefs_mkdir(sb, root, buf);
+		dir = oprofilefs_mkdir(root, buf);
 		if (!dir)
 			continue;
-		oprofilefs_create_ulong(sb, dir, "enabled",
+		oprofilefs_create_ulong(dir, "enabled",
 					&counter_config[i].enabled);
-		oprofilefs_create_ulong(sb, dir, "event",
+		oprofilefs_create_ulong(dir, "event",
 					&counter_config[i].event);
-		oprofilefs_create_ulong(sb, dir, "count",
+		oprofilefs_create_ulong(dir, "count",
 					&counter_config[i].count);
-		oprofilefs_create_ulong(sb, dir, "unit_mask",
+		oprofilefs_create_ulong(dir, "unit_mask",
 					&counter_config[i].unit_mask);
-		oprofilefs_create_ulong(sb, dir, "kernel",
+		oprofilefs_create_ulong(dir, "kernel",
 					&counter_config[i].kernel);
-		oprofilefs_create_ulong(sb, dir, "user",
+		oprofilefs_create_ulong(dir, "user",
 					&counter_config[i].user);
 	}
 
--- a/drivers/acpi/acpi_pad-xen.c
+++ b/drivers/acpi/acpi_pad-xen.c
@@ -162,7 +162,6 @@ static void acpi_pad_notify(acpi_handle
 	switch (event) {
 	case ACPI_PROCESSOR_AGGREGATOR_NOTIFY:
 		acpi_pad_handle_notify(handle);
-		acpi_bus_generate_proc_event(device, event, 0);
 		acpi_bus_generate_netlink_event(device->pnp.device_class,
 			dev_name(&device->dev), event, 0);
 		break;
--- a/drivers/char/tpm/Kconfig
+++ b/drivers/char/tpm/Kconfig
@@ -112,8 +112,8 @@ config TCG_ST33_I2C
 
 config TCG_XEN
 	tristate "XEN TPM Interface"
-	depends on TCG_TPM && XEN
-	select XEN_XENBUS_FRONTEND
+	depends on PARAVIRT_XEN || XEN
+	select XEN_XENBUS_FRONTEND if PARAVIRT_XEN
 	---help---
 	  If you want to make TPM support available to a Xen user domain,
 	  say Yes and it will be accessible from within Linux. See
@@ -122,11 +122,11 @@ config TCG_XEN
 	  To compile this driver as a module, choose M here; the module
 	  will be called xen-tpmfront.
 
-config TCG_XEN
-	tristate "XEN TPM Interface"
+config TCG_XEN_V1
+	tristate "XEN TPM Interface (v1, deprecated)"
 	depends on XEN
 	---help---
-	  If you want to make TPM support available to a Xen user domain,
+	  If you want to make TPM v1 support available to a Xen user domain,
 	  say Yes and it will be accessible from within Linux.
 	  To compile this driver as a module, choose M here; the module
 	  will be called tpm_xenu.
--- a/drivers/char/tpm/Makefile
+++ b/drivers/char/tpm/Makefile
@@ -22,5 +22,5 @@ obj-$(CONFIG_TCG_INFINEON) += tpm_infine
 obj-$(CONFIG_TCG_IBMVTPM) += tpm_ibmvtpm.o
 obj-$(CONFIG_TCG_ST33_I2C) += tpm_i2c_stm_st33.o
 obj-$(CONFIG_TCG_XEN) += xen-tpmfront.o
-obj-$(CONFIG_TCG_XEN) += tpm_xenu.o
+obj-$(CONFIG_TCG_XEN_V1) += tpm_xenu.o
 tpm_xenu-y = tpm_xen.o tpm_vtpm.o
--- a/drivers/char/tpm/xen-tpmfront.c
+++ b/drivers/char/tpm/xen-tpmfront.c
@@ -11,19 +11,26 @@
 #include <linux/err.h>
 #include <linux/interrupt.h>
 #include <xen/xen.h>
-#include <xen/events.h>
 #include <xen/interface/io/tpmif.h>
+#ifdef CONFIG_PARAVIRT_XEN
+#include <xen/events.h>
 #include <xen/grant_table.h>
-#include <xen/xenbus.h>
 #include <xen/page.h>
-#include "tpm.h"
 #include <xen/platform_pci.h>
+#else
+#include <xen/evtchn.h>
+#define bind_evtchn_to_irqhandler bind_caller_port_to_irqhandler
+#include <xen/gnttab.h>
+#define gnttab_end_foreign_access(g, r, p) gnttab_end_foreign_access(g, p)
+#endif
+#include <xen/xenbus.h>
+#include "tpm.h"
 
 struct tpm_private {
 	struct tpm_chip *chip;
 	struct xenbus_device *dev;
 
-	struct vtpm_shared_page *shr;
+	struct tpmif_shared_page *shr;
 
 	unsigned int evtchn;
 	int ring_ref;
@@ -41,12 +48,12 @@ static u8 vtpm_status(struct tpm_chip *c
 {
 	struct tpm_private *priv = TPM_VPRIV(chip);
 	switch (priv->shr->state) {
-	case VTPM_STATE_IDLE:
+	case TPMIF_STATE_IDLE:
 		return VTPM_STATUS_IDLE | VTPM_STATUS_CANCELED;
-	case VTPM_STATE_FINISH:
+	case TPMIF_STATE_FINISH:
 		return VTPM_STATUS_IDLE | VTPM_STATUS_RESULT;
-	case VTPM_STATE_SUBMIT:
-	case VTPM_STATE_CANCEL: /* cancel requested, not yet canceled */
+	case TPMIF_STATE_SUBMIT:
+	case TPMIF_STATE_CANCEL: /* cancel requested, not yet canceled */
 		return VTPM_STATUS_RUNNING;
 	default:
 		return 0;
@@ -61,12 +68,12 @@ static bool vtpm_req_canceled(struct tpm
 static void vtpm_cancel(struct tpm_chip *chip)
 {
 	struct tpm_private *priv = TPM_VPRIV(chip);
-	priv->shr->state = VTPM_STATE_CANCEL;
+	priv->shr->state = TPMIF_STATE_CANCEL;
 	wmb();
 	notify_remote_via_evtchn(priv->evtchn);
 }
 
-static unsigned int shr_data_offset(struct vtpm_shared_page *shr)
+static unsigned int shr_data_offset(struct tpmif_shared_page *shr)
 {
 	return sizeof(*shr) + sizeof(u32) * shr->nr_extra_pages;
 }
@@ -74,7 +81,7 @@ static unsigned int shr_data_offset(stru
 static int vtpm_send(struct tpm_chip *chip, u8 *buf, size_t count)
 {
 	struct tpm_private *priv = TPM_VPRIV(chip);
-	struct vtpm_shared_page *shr = priv->shr;
+	struct tpmif_shared_page *shr = priv->shr;
 	unsigned int offset = shr_data_offset(shr);
 
 	u32 ordinal;
@@ -96,7 +103,7 @@ static int vtpm_send(struct tpm_chip *ch
 	memcpy(offset + (u8 *)shr, buf, count);
 	shr->length = count;
 	barrier();
-	shr->state = VTPM_STATE_SUBMIT;
+	shr->state = TPMIF_STATE_SUBMIT;
 	wmb();
 	notify_remote_via_evtchn(priv->evtchn);
 
@@ -116,11 +123,11 @@ static int vtpm_send(struct tpm_chip *ch
 static int vtpm_recv(struct tpm_chip *chip, u8 *buf, size_t count)
 {
 	struct tpm_private *priv = TPM_VPRIV(chip);
-	struct vtpm_shared_page *shr = priv->shr;
+	struct tpmif_shared_page *shr = priv->shr;
 	unsigned int offset = shr_data_offset(shr);
 	size_t length = shr->length;
 
-	if (shr->state == VTPM_STATE_IDLE)
+	if (shr->state == TPMIF_STATE_IDLE)
 		return -ECANCELED;
 
 	/* In theory the wait at the end of _send makes this one unnecessary */
@@ -159,12 +166,12 @@ static irqreturn_t tpmif_interrupt(int d
 	struct tpm_private *priv = dev_id;
 
 	switch (priv->shr->state) {
-	case VTPM_STATE_IDLE:
-	case VTPM_STATE_FINISH:
+	case TPMIF_STATE_IDLE:
+	case TPMIF_STATE_FINISH:
 		wake_up_interruptible(&priv->chip->vendor.read_queue);
 		break;
-	case VTPM_STATE_SUBMIT:
-	case VTPM_STATE_CANCEL:
+	case TPMIF_STATE_SUBMIT:
+	case TPMIF_STATE_CANCEL:
 	default:
 		break;
 	}
@@ -239,7 +246,7 @@ static int setup_ring(struct xenbus_devi
 		goto abort_transaction;
 	}
 
-	rv = xenbus_printf(xbt, dev->nodename, "feature-protocol-v2", "1");
+	rv = xenbus_write(xbt, dev->nodename, "feature-protocol-v2", "1");
 	if (rv) {
 		message = "writing feature-protocol-v2";
 		goto abort_transaction;
--- a/drivers/cpuidle/Kconfig
+++ b/drivers/cpuidle/Kconfig
@@ -1,8 +1,8 @@
 menu "CPU Idle"
+	depends on !PROCESSOR_EXTERNAL_CONTROL
 
 config CPU_IDLE
 	bool "CPU idle PM support"
-	depends on !PROCESSOR_EXTERNAL_CONTROL
 	default y if ACPI || PPC_PSERIES
 	select CPU_IDLE_GOV_LADDER if (!NO_HZ && !NO_HZ_IDLE)
 	select CPU_IDLE_GOV_MENU if (NO_HZ || NO_HZ_IDLE)
--- a/drivers/hwmon/coretemp-xen.c
+++ b/drivers/hwmon/coretemp-xen.c
@@ -324,6 +324,18 @@ static int adjust_tjmax(struct platform_
 	return tjmax;
 }
 
+static bool cpu_has_tjmax(struct platform_data *c)
+{
+	u8 model = c->x86_model;
+
+	return model > 0xe &&
+	       model != 0x1c &&
+	       model != 0x26 &&
+	       model != 0x27 &&
+	       model != 0x35 &&
+	       model != 0x36;
+}
+
 static int get_tjmax(struct platform_data *c, u32 id, struct device *dev)
 {
 	int err;
@@ -336,7 +348,7 @@ static int get_tjmax(struct platform_dat
 	 */
 	err = rdmsr_safe_on_pcpu(id, MSR_IA32_TEMPERATURE_TARGET, &eax, &edx);
 	if (err < 0) {
-		if (c->x86_model > 0xe && c->x86_model != 0x1c)
+		if (cpu_has_tjmax(c))
 			dev_warn(dev, "Unable to read TjMax from CPU %u\n", id);
 	} else {
 		val = (eax >> 16) & 0xff;
--- a/drivers/oprofile/oprofile_files.c
+++ b/drivers/oprofile/oprofile_files.c
@@ -327,8 +327,8 @@ void oprofile_create_files(struct dentry
 	oprofilefs_create_file(root, "enable", &enable_fops);
 	oprofilefs_create_file_perm(root, "dump", &dump_fops, 0666);
 #ifdef CONFIG_XEN
-	oprofilefs_create_file(sb, root, "active_domains", &active_domain_ops);
-	oprofilefs_create_file(sb, root, "passive_domains", &passive_domain_ops);
+	oprofilefs_create_file(root, "active_domains", &active_domain_ops);
+	oprofilefs_create_file(root, "passive_domains", &passive_domain_ops);
 #endif
 	oprofilefs_create_file(root, "buffer", &event_buffer_fops);
 	oprofilefs_create_ulong(root, "buffer_size", &oprofile_buffer_size);
--- a/drivers/pci/msi-xen.c
+++ b/drivers/pci/msi-xen.c
@@ -60,12 +60,10 @@ struct msi_dev_list {
 
 /* Arch hooks */
 
-#ifndef arch_msi_check_device
 int arch_msi_check_device(struct pci_dev *dev, int nvec, int type)
 {
 	return 0;
 }
-#endif
 
 static void msi_set_enable(struct pci_dev *dev, int enable)
 {
--- a/drivers/scsi/scsi_error.c
+++ b/drivers/scsi/scsi_error.c
@@ -1930,19 +1930,6 @@ int scsi_decide_disposition(struct scsi_
 		   * Overall, shouldn't the return value of this function be
 		   * the same when called twice in immediate succession?
 		   */
-	case DID_TARGET_FAILURE:
-		/*
-		 * scsi_check_sense(scmd) returning TARGET_ERROR gets
-		 * converted to DID_TARGET_FAILURE below, so if that
-		 * happened on the backend side, the frontend side
-		 * handling here would otherwise cause error handling to be
-		 * invoked from scsi_softirq_done().
-		 */
-		if (msg_byte(scmd->result) == COMMAND_COMPLETE &&
-		    status_byte(scmd->result) == CHECK_CONDITION &&
-		    scsi_check_sense(scmd) == TARGET_ERROR)
-			return SUCCESS;
-		return FAILED;
 	case DID_NEXUS_FAILURE:
 		/* Similarly for the respective conversion above/below. */
 		if (msg_byte(scmd->result) == COMMAND_COMPLETE &&
--- a/drivers/xen/blkback/xenbus.c
+++ b/drivers/xen/blkback/xenbus.c
@@ -367,7 +367,8 @@ static void backend_changed(struct xenbu
 	}
 
 	/* Front end dir is a number, which is used as the handle. */
-	handle = simple_strtoul(strrchr(dev->otherend, '/') + 1, NULL, 0);
+	if (kstrtoul(strrchr(dev->otherend, '/') + 1, 0, &handle))
+		return;
 
 	be->major = major;
 	be->minor = minor;
--- a/drivers/xen/evtchn.c
+++ b/drivers/xen/evtchn.c
@@ -337,7 +337,7 @@ static void evtchn_unbind_from_user(stru
 
 	unbind_from_irqhandler(irq, evtchn);
 #ifdef CONFIG_XEN
-	WARN_ON(close_evtchn(port));
+	WARN_ON(close_evtchn(evtchn->port));
 #endif
 
 	del_evtchn(u, evtchn);
--- a/drivers/xen/usbback/xenbus.c
+++ b/drivers/xen/usbback/xenbus.c
@@ -168,7 +168,7 @@ static int usbback_probe(struct xenbus_d
 			  const struct xenbus_device_id *id)
 {
 	usbif_t *usbif;
-	unsigned int handle;
+	unsigned long handle;
 	int num_ports;
 	int usb_ver;
 	int err;
@@ -176,7 +176,10 @@ static int usbback_probe(struct xenbus_d
 	if (usb_disabled())
 		return -ENODEV;
 
-	handle = simple_strtoul(strrchr(dev->otherend, '/') + 1, NULL, 0);
+	err = kstrtoul(strrchr(dev->otherend, '/') + 1, 0, &handle);
+	if (err)
+		return err;
+
 	usbif = usbif_alloc(dev->otherend_id, handle);
 	if (!usbif) {
 		xenbus_dev_fatal(dev, -ENOMEM, "allocating backend interface");
--- a/include/linux/msi.h
+++ b/include/linux/msi.h
@@ -74,6 +74,7 @@ void default_restore_msi_irqs(struct pci
 u32 default_msi_mask_irq(struct msi_desc *desc, u32 mask, u32 flag);
 u32 default_msix_mask_irq(struct msi_desc *desc, u32 flag);
 
+#ifndef CONFIG_XEN
 struct msi_chip {
 	struct module *owner;
 	struct device *dev;
@@ -86,5 +87,6 @@ struct msi_chip {
 	int (*check_device)(struct msi_chip *chip, struct pci_dev *dev,
 			    int nvec, int type);
 };
+#endif
 
 #endif /* LINUX_MSI_H */
--- a/include/xen/balloon.h
+++ b/include/xen/balloon.h
@@ -86,6 +86,9 @@ int alloc_xenballooned_pages(int nr_page
 		bool highmem);
 void free_xenballooned_pages(int nr_pages, struct page **pages);
 
+struct page *get_balloon_scratch_page(void);
+void put_balloon_scratch_page(void);
+
 #endif /* CONFIG_PARAVIRT_XEN */
 
 struct device;
--- a/lib/swiotlb-xen.c
+++ b/lib/swiotlb-xen.c
@@ -737,13 +737,13 @@ swiotlb_map_sg_attrs(struct device *hwde
 				swiotlb_full(hwdev, sg->length, dir, 0);
 				swiotlb_unmap_sg_attrs(hwdev, sgl, i, dir,
 						       attrs);
-				sgl[0].dma_length = 0;
+				sg_dma_len(sgl) = 0;
 				return 0;
 			}
 			sg->dma_address = phys_to_dma(hwdev, map);
 		} else
 			sg->dma_address = dev_addr;
-		sg->dma_length = sg->length;
+		sg_dma_len(sg) = sg->length;
 	}
 	return nelems;
 }
@@ -771,7 +771,7 @@ swiotlb_unmap_sg_attrs(struct device *hw
 	BUG_ON(dir == DMA_NONE);
 
 	for_each_sg(sgl, sg, nelems, i)
-		unmap_single(hwdev, sg->dma_address, sg->dma_length, dir);
+		unmap_single(hwdev, sg->dma_address, sg_dma_len(sg), dir);
 
 }
 EXPORT_SYMBOL(swiotlb_unmap_sg_attrs);
@@ -801,7 +801,7 @@ swiotlb_sync_sg(struct device *hwdev, st
 
 	for_each_sg(sgl, sg, nelems, i)
 		swiotlb_sync_single(hwdev, sg->dma_address,
-				    sg->dma_length, dir, target);
+				    sg_dma_len(sg), dir, target);
 }
 
 void
