From: Linux Kernel Mailing List <linux-kernel@vger.kernel.org>
Subject: Linux: 3.13
Patch-mainline: 3.13

 This patch contains the differences between 3.12 and 3.13.

Automatically created from "patch-3.13" by xen-port-patches.py
Acked-by: jbeulich@suse.com

--- a/arch/x86/Kconfig.debug
+++ b/arch/x86/Kconfig.debug
@@ -62,7 +62,7 @@ config EARLY_PRINTK_DBGP
 
 config EARLY_PRINTK_EFI
 	bool "Early printk via the EFI framebuffer"
-	depends on EFI && EARLY_PRINTK
+	depends on EARLY_PRINTK && EFI && !XEN
 	select FONT_SUPPORT
 	---help---
 	  Write kernel log output directly into the EFI framebuffer.
--- a/arch/x86/include/mach-xen/asm/desc.h
+++ b/arch/x86/include/mach-xen/asm/desc.h
@@ -352,10 +352,25 @@ static inline void write_trace_idt_entry
 {
 	write_idt_entry(trace_idt_table, entry, gate);
 }
+
+static inline void _trace_set_gate(int gate, unsigned type, void *addr,
+				   unsigned dpl, unsigned ist, unsigned seg)
+{
+	gate_desc s;
+
+	pack_gate(&s, type, (unsigned long)addr, dpl, ist, seg);
+	/*
+	 * does not need to be atomic because it is only done once at
+	 * setup time
+	 */
+	write_trace_idt_entry(gate, &s);
+}
 #else
 static inline void write_trace_idt_entry(int entry, const gate_desc *gate)
 {
 }
+
+#define _trace_set_gate(gate, type, addr, dpl, ist, seg)
 #endif
 
 static inline void _set_gate(int gate, unsigned type, void *addr,
@@ -378,11 +393,14 @@ static inline void _set_gate(int gate, u
  * Pentium F0 0F bugfix can have resulted in the mapped
  * IDT being write-protected.
  */
-static inline void set_intr_gate(unsigned int n, void *addr)
-{
-	BUG_ON((unsigned)n > 0xFF);
-	_set_gate(n, GATE_INTERRUPT, addr, 0, 0, __KERNEL_CS);
-}
+#define set_intr_gate(n, addr)						\
+	do {								\
+		BUG_ON((unsigned)n > 0xFF);				\
+		_set_gate(n, GATE_INTERRUPT, (void *)addr, 0, 0,	\
+			  __KERNEL_CS);					\
+		_trace_set_gate(n, GATE_INTERRUPT, (void *)trace_##addr,\
+				0, 0, __KERNEL_CS);			\
+	} while (0)
 
 extern int first_system_vector;
 /* used_vectors is BITMAP for irq is not managed by percpu vector_irq */
@@ -399,37 +417,10 @@ static inline void alloc_system_vector(i
 	}
 }
 
-#ifdef CONFIG_TRACING
-static inline void trace_set_intr_gate(unsigned int gate, void *addr)
-{
-	gate_desc s;
-
-	pack_gate(&s, GATE_INTERRUPT, (unsigned long)addr, 0, 0, __KERNEL_CS);
-	write_idt_entry(trace_idt_table, gate, &s);
-}
-
-static inline void __trace_alloc_intr_gate(unsigned int n, void *addr)
-{
-	trace_set_intr_gate(n, addr);
-}
-#else
-static inline void trace_set_intr_gate(unsigned int gate, void *addr)
-{
-}
-
-#define __trace_alloc_intr_gate(n, addr)
-#endif
-
-static inline void __alloc_intr_gate(unsigned int n, void *addr)
-{
-	set_intr_gate(n, addr);
-}
-
 #define alloc_intr_gate(n, addr)				\
 	do {							\
 		alloc_system_vector(n);				\
-		__alloc_intr_gate(n, addr);			\
-		__trace_alloc_intr_gate(n, trace_##addr);	\
+		set_intr_gate(n, addr);				\
 	} while (0)
 
 /*
@@ -537,6 +528,13 @@ static inline void load_current_idt(void
 	else
 		load_idt((const struct desc_ptr *)&idt_descr);
 }
+#elif defined(CONFIG_TRACING)
+void load_current_trap_table(void);
+extern atomic_t trace_trap_table_ctr;
+static inline bool is_trace_trap_table_enabled(void)
+{
+	return atomic_read(&trace_trap_table_ctr);
+}
 #endif /* !CONFIG_X86_NO_IDT */
 
 #endif /* _ASM_X86_DESC_H */
--- a/arch/x86/include/mach-xen/asm/fpu-internal.h
+++ b/arch/x86/include/mach-xen/asm/fpu-internal.h
@@ -31,7 +31,7 @@ static inline fpu_switch_t xen_switch_fp
 	 * or if the past 5 consecutive context-switches used math.
 	 */
 	fpu.preload = tsk_used_math(new) && (use_eager_fpu() ||
-					     new->fpu_counter > 5);
+					     new->thread.fpu_counter > 5);
 	if (__thread_has_fpu(old)) {
 		if (!__save_init_fpu(old))
 			cpu = ~0;
@@ -40,7 +40,7 @@ static inline fpu_switch_t xen_switch_fp
 
 		/* Don't change CR0.TS if we just switch! */
 		if (fpu.preload) {
-			new->fpu_counter++;
+			new->thread.fpu_counter++;
 			__thread_set_has_fpu(new);
 			prefetch(new->thread.fpu.state);
 		} else if (!use_eager_fpu()) {
@@ -48,10 +48,10 @@ static inline fpu_switch_t xen_switch_fp
 			(*mcl)++->args[0] = 1;
 		}
 	} else {
-		old->fpu_counter = 0;
+		old->thread.fpu_counter = 0;
 		old->thread.fpu.last_cpu = ~0;
 		if (fpu.preload) {
-			new->fpu_counter++;
+			new->thread.fpu_counter++;
 			if (!use_eager_fpu() && fpu_lazy_restore(new, cpu))
 				fpu.preload = 0;
 			else
--- a/arch/x86/include/mach-xen/asm/pci.h
+++ b/arch/x86/include/mach-xen/asm/pci.h
@@ -15,7 +15,7 @@ struct pci_sysdata {
 	int		domain;		/* PCI domain */
 	int		node;		/* NUMA node */
 #ifdef CONFIG_ACPI
-	void		*acpi;		/* ACPI-specific data */
+	struct acpi_device *companion;	/* ACPI companion device */
 #endif
 #if defined(CONFIG_X86_64) && !defined(CONFIG_XEN)
 	void		*iommu;		/* IOMMU private data */
--- a/arch/x86/include/mach-xen/asm/pgtable.h
+++ b/arch/x86/include/mach-xen/asm/pgtable.h
@@ -454,9 +454,16 @@ static inline int pte_present(pte_t a)
 }
 
 #define pte_accessible pte_accessible
-static inline int pte_accessible(pte_t a)
+static inline bool pte_accessible(struct mm_struct *mm, pte_t a)
 {
-	return pte_flags(a) & _PAGE_PRESENT;
+	if (pte_flags(a) & _PAGE_PRESENT)
+		return true;
+
+	if ((pte_flags(a) & (_PAGE_PROTNONE | _PAGE_NUMA)) &&
+			mm_tlb_flush_pending(mm))
+		return true;
+
+	return false;
 }
 
 static inline int pte_hidden(pte_t pte)
--- a/arch/x86/include/mach-xen/asm/processor.h
+++ b/arch/x86/include/mach-xen/asm/processor.h
@@ -502,6 +502,15 @@ struct thread_struct {
 	unsigned long		iopl;
 	/* Max allowed port in the bitmap, in bytes: */
 	unsigned		io_bitmap_max;
+	/*
+	 * fpu_counter contains the number of consecutive context switches
+	 * that the FPU is used. If this is over a threshold, the lazy fpu
+	 * saving becomes unlazy to save the trap. This is an unsigned char
+	 * so that after 256 times the counter wraps and the behavior turns
+	 * lazy again; this to deal with bursty apps that only use FPU for
+	 * a short time
+	 */
+	unsigned char fpu_counter;
 };
 
 /*
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -122,6 +122,5 @@ ifeq ($(CONFIG_X86_64),y)
 endif
 
 disabled-obj-$(CONFIG_XEN) := crash.o early-quirks.o i8237.o i8253.o i8259.o \
-	irqinit.o pci-swiotlb.o reboot.o smpboot.o tracepoint.o trampoline%.o \
-	tsc%.o vsmp%.o
+	irqinit.o pci-swiotlb.o reboot.o smpboot.o trampoline%.o tsc%.o vsmp%.o
 disabled-obj-$(CONFIG_XEN_UNPRIVILEGED_GUEST) += probe_roms.o
--- a/arch/x86/kernel/acpi/boot.c
+++ b/arch/x86/kernel/acpi/boot.c
@@ -198,7 +198,13 @@ static int __init acpi_parse_madt(struct
  *
  * Returns the logic cpu number which maps to the local apic
  */
-static int acpi_register_lapic(int id, u8 enabled)
+static
+#ifndef CONFIG_XEN
+int
+#else
+void
+#endif
+acpi_register_lapic(int id, u8 enabled)
 {
 #ifndef CONFIG_XEN
 	unsigned int ver = 0;
@@ -643,7 +649,7 @@ static int _acpi_map_lsapic(acpi_handle
 	return 0;
 }
 #else
-#define _acpi_map_lsapic(h, p) (-EINVAL)
+#define _acpi_map_lsapic(h, p, c) (-EINVAL)
 #endif
 
 /* wrapper to silence section mismatch warning */
--- a/arch/x86/kernel/cpu/common-xen.c
+++ b/arch/x86/kernel/cpu/common-xen.c
@@ -366,7 +366,8 @@ static void filter_cpuid_features(struct
 /* Look up CPU names by table lookup. */
 static const char *table_lookup_model(struct cpuinfo_x86 *c)
 {
-	const struct cpu_model_info *info;
+#ifdef CONFIG_X86_32
+	const struct legacy_cpu_model_info *info;
 
 	if (c->x86_model >= 16)
 		return NULL;	/* Range check */
@@ -374,13 +375,14 @@ static const char *table_lookup_model(st
 	if (!this_cpu)
 		return NULL;
 
-	info = this_cpu->c_models;
+	info = this_cpu->legacy_models;
 
-	while (info && info->family) {
+	while (info->family) {
 		if (info->family == c->x86)
 			return info->model_names[c->x86_model];
 		info++;
 	}
+#endif
 	return NULL;		/* Not found */
 }
 
@@ -492,8 +494,8 @@ void cpu_detect_cache_sizes(struct cpuin
 	c->x86_tlbsize += ((ebx >> 16) & 0xfff) + (ebx & 0xfff);
 #else
 	/* do processor-specific cache resizing */
-	if (this_cpu->c_size_cache)
-		l2size = this_cpu->c_size_cache(c, l2size);
+	if (this_cpu->legacy_cache_size)
+		l2size = this_cpu->legacy_cache_size(c, l2size);
 
 	/* Allow user to override all this if necessary. */
 	if (cachesize_override != -1)
@@ -1164,6 +1166,9 @@ DEFINE_PER_CPU(char *, irq_stack_ptr) =
 
 DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;
 
+DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;
+EXPORT_PER_CPU_SYMBOL(__preempt_count);
+
 DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
 
 #ifndef CONFIG_X86_NO_TSS
@@ -1258,6 +1263,8 @@ void debug_stack_reset(void)
 
 DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
+DEFINE_PER_CPU(int, __preempt_count) = INIT_PREEMPT_COUNT;
+EXPORT_PER_CPU_SYMBOL(__preempt_count);
 DEFINE_PER_CPU(struct task_struct *, fpu_owner_task);
 
 #ifdef CONFIG_CC_STACKPROTECTOR
--- a/arch/x86/kernel/early_printk-xen.c
+++ b/arch/x86/kernel/early_printk-xen.c
@@ -13,9 +13,10 @@
 #include <asm/setup.h>
 #include <asm/pci-direct.h>
 #include <asm/fixmap.h>
-#include <asm/mrst.h>
+#include <asm/intel-mid.h>
 #include <asm/pgtable.h>
 #include <linux/usb/ehci_def.h>
+#include <linux/efi.h>
 
 #ifndef CONFIG_XEN
 /* Simple VGA output */
@@ -243,7 +244,8 @@ static int __init setup_early_printk(cha
 			max_ypos = boot_params.screen_info.orig_video_lines;
 			current_ypos = boot_params.screen_info.orig_y;
 #else
-		if (!strncmp(buf, "vga", 3) || !strncmp(buf, "xen", 3)) {
+		if (!strncmp(buf, efi_enabled(EFI_BOOT) ? "efi" : "vga", 3)
+		    || !strncmp(buf, "xen", 3)) {
 #endif
 			early_console_register(&early_vga_console, keep);
 		}
--- a/arch/x86/kernel/entry_32-xen.S
+++ b/arch/x86/kernel/entry_32-xen.S
@@ -366,12 +366,9 @@ END(ret_from_exception)
 #ifdef CONFIG_PREEMPT
 ENTRY(resume_kernel)
 	DISABLE_INTERRUPTS(CLBR_ANY)
-	cmpl $0,TI_preempt_count(%ebp)	# non-zero preempt_count ?
-	jnz restore_all
 need_resched:
-	movl TI_flags(%ebp), %ecx	# need_resched set ?
-	testb $_TIF_NEED_RESCHED, %cl
-	jz restore_all
+	cmpl $0,PER_CPU_VAR(__preempt_count)
+	jnz restore_all
 	testl $X86_EFLAGS_IF,PT_EFLAGS(%esp)	# interrupts off (exception path) ?
 	jz restore_all
 	call preempt_schedule_irq
@@ -1205,7 +1202,7 @@ ENTRY(ftrace_caller)
 	pushl $0	/* Pass NULL as regs pointer */
 	movl 4*4(%esp), %eax
 	movl 0x4(%ebp), %edx
-	leal function_trace_op, %ecx
+	movl function_trace_op, %ecx
 	subl $MCOUNT_INSN_SIZE, %eax
 
 .globl ftrace_call
@@ -1263,7 +1260,7 @@ ENTRY(ftrace_regs_caller)
 	movl 12*4(%esp), %eax	/* Load ip (1st parameter) */
 	subl $MCOUNT_INSN_SIZE, %eax	/* Adjust ip */
 	movl 0x4(%ebp), %edx	/* Load parent ip (2nd parameter) */
-	leal function_trace_op, %ecx /* Save ftrace_pos in 3rd parameter */
+	movl function_trace_op, %ecx /* Save ftrace_pos in 3rd parameter */
 	pushl %esp		/* Save pt_regs as 4th parameter */
 
 GLOBAL(ftrace_regs_call)
@@ -1490,6 +1487,16 @@ mask=0
  */
 	.pushsection .kprobes.text, "ax"
 
+#ifdef CONFIG_TRACING
+ENTRY(trace_page_fault)
+	RING0_EC_FRAME
+	ASM_CLAC
+	pushl_cfi $trace_do_page_fault
+	jmp error_code
+	CFI_ENDPROC
+END(trace_page_fault)
+#endif
+
 ENTRY(page_fault)
 	RING0_EC_FRAME
 	ASM_CLAC
--- a/arch/x86/kernel/entry_64-xen.S
+++ b/arch/x86/kernel/entry_64-xen.S
@@ -93,7 +93,7 @@ END(function_hook)
 	MCOUNT_SAVE_FRAME \skip
 
 	/* Load the ftrace_ops into the 3rd parameter */
-	leaq function_trace_op, %rdx
+	movq function_trace_op(%rip), %rdx
 
 	/* Load ip into the first parameter */
 	movq RIP(%rsp), %rdi
@@ -992,10 +992,8 @@ retint_signal:
 	/* Returning to kernel space. Check if we need preemption */
 	/* rcx:	 threadinfo. interrupts off. */
 ENTRY(retint_kernel)
-	cmpl $0,TI_preempt_count(%rcx)
+	cmpl $0,PER_CPU_VAR(__preempt_count)
 	jnz  retint_restore_args
-	bt  $TIF_NEED_RESCHED,TI_flags(%rcx)
-	jnc  retint_restore_args
 	bt   $9,EFLAGS-ARGOFFSET(%rsp)	/* interrupts off? */
 	jnc  retint_restore_args
 	call preempt_schedule_irq
@@ -1058,6 +1056,17 @@ ENTRY(\sym)
 END(\sym)
 .endm
 
+#ifdef CONFIG_TRACING
+.macro trace_errorentry sym do_sym
+errorentry trace_\sym trace_\do_sym
+errorentry \sym \do_sym
+.endm
+#else
+.macro trace_errorentry sym do_sym
+errorentry \sym \do_sym
+.endm
+#endif
+
 	/* error code is on the stack already */
 .macro paranoiderrorentry sym do_sym
 	errorentry \sym \do_sym
@@ -1186,7 +1195,7 @@ errorentry alignment_check do_alignment_
 zeroentry simd_coprocessor_error do_simd_coprocessor_error
 
 /* Call softirq on interrupt stack. Interrupts are off. */
-ENTRY(call_softirq)
+ENTRY(do_softirq_own_stack)
 	CFI_STARTPROC
 	pushq_cfi %rbp
 	CFI_REL_OFFSET rbp,0
@@ -1203,7 +1212,7 @@ ENTRY(call_softirq)
 	decl PER_CPU_VAR(irq_count)
 	ret
 	CFI_ENDPROC
-END(call_softirq)
+END(do_softirq_own_stack)
 
 /*
  * Some functions should be protected against kprobes
@@ -1215,7 +1224,7 @@ zeroentry nmi do_nmi_callback
 paranoidzeroentry_ist int3 do_int3 DEBUG_STACK
 paranoiderrorentry stack_segment do_stack_segment
 errorentry general_protection do_general_protection
-errorentry page_fault do_page_fault
+trace_errorentry page_fault do_page_fault
 #ifdef CONFIG_KVM_GUEST
 errorentry async_page_fault do_async_page_fault
 #endif
--- a/arch/x86/kernel/head32-xen.c
+++ b/arch/x86/kernel/head32-xen.c
@@ -53,8 +53,8 @@ asmlinkage void __init i386_start_kernel
 
 	/* Call the subarch specific early setup function */
 	switch (boot_params.hdr.hardware_subarch) {
-	case X86_SUBARCH_MRST:
-		x86_mrst_early_setup();
+	case X86_SUBARCH_INTEL_MID:
+		x86_intel_mid_early_setup();
 		break;
 	case X86_SUBARCH_CE4100:
 		x86_ce4100_early_setup();
--- a/arch/x86/kernel/head64-xen.c
+++ b/arch/x86/kernel/head64-xen.c
@@ -180,7 +180,7 @@ asmlinkage void __init x86_64_start_kern
 	clear_bss();
 
 	for (i = 0; i < NUM_EXCEPTION_VECTORS; i++)
-		set_intr_gate(i, &early_idt_handlers[i]);
+		set_intr_gate(i, early_idt_handlers[i]);
 	load_idt((const struct desc_ptr *)&idt_descr);
 
 	copy_bootdata(__va(real_mode_data));
--- a/arch/x86/kernel/process-xen.c
+++ b/arch/x86/kernel/process-xen.c
@@ -383,9 +383,9 @@ static void amd_e400_idle(void)
 		 * The switch back from broadcast mode needs to be
 		 * called with interrupts disabled.
 		 */
-		 local_irq_disable();
-		 clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &cpu);
-		 local_irq_enable();
+		local_irq_disable();
+		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &cpu);
+		local_irq_enable();
 	} else
 		default_idle();
 }
--- a/arch/x86/kernel/process_32-xen.c
+++ b/arch/x86/kernel/process_32-xen.c
@@ -156,7 +156,7 @@ int copy_thread(unsigned long clone_flag
 		childregs->orig_ax = -1;
 		childregs->cs = __KERNEL_CS | get_kernel_rpl();
 		childregs->flags = X86_EFLAGS_IF | X86_EFLAGS_FIXED;
-		p->fpu_counter = 0;
+		p->thread.fpu_counter = 0;
 		p->thread.io_bitmap_ptr = NULL;
 		memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));
 		return 0;
@@ -169,7 +169,7 @@ int copy_thread(unsigned long clone_flag
 	p->thread.ip = (unsigned long) ret_from_fork;
 	task_user_gs(p) = get_user_gs(current_pt_regs());
 
-	p->fpu_counter = 0;
+	p->thread.fpu_counter = 0;
 	p->thread.io_bitmap_ptr = NULL;
 	tsk = current;
 	err = -ENOMEM;
@@ -355,6 +355,14 @@ __switch_to(struct task_struct *prev_p,
 	}
 
 	/*
+	 * If it were not for PREEMPT_ACTIVE we could guarantee that the
+	 * preempt_count of all tasks was equal here and this would not be
+	 * needed.
+	 */
+	task_thread_info(prev_p)->saved_preempt_count = this_cpu_read(__preempt_count);
+	this_cpu_write(__preempt_count, task_thread_info(next_p)->saved_preempt_count);
+
+	/*
 	 * Now maybe handle debug registers
 	 */
 	if (unlikely(task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV ||
--- a/arch/x86/kernel/process_64-xen.c
+++ b/arch/x86/kernel/process_64-xen.c
@@ -66,7 +66,7 @@ void __show_regs(struct pt_regs *regs, i
 	unsigned int ds, cs, es;
 
 	printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
-	printk_address(regs->ip, 1);
+	printk_address(regs->ip);
 	printk(KERN_DEFAULT "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,
 			regs->sp, regs->flags);
 	printk(KERN_DEFAULT "RAX: %016lx RBX: %016lx RCX: %016lx\n",
@@ -171,7 +171,7 @@ int copy_thread(unsigned long clone_flag
 	childregs = task_pt_regs(p);
 	p->thread.sp = (unsigned long) childregs;
 	set_tsk_thread_flag(p, TIF_FORK);
-	p->fpu_counter = 0;
+	p->thread.fpu_counter = 0;
 	p->thread.io_bitmap_ptr = NULL;
 
 	savesegment(gs, p->thread.gsindex);
@@ -424,6 +424,14 @@ __switch_to(struct task_struct *prev_p,
 	 */
 	this_cpu_write(current_task, next_p);
 
+	/*
+	 * If it were not for PREEMPT_ACTIVE we could guarantee that the
+	 * preempt_count of all tasks was equal here and this would not be
+	 * needed.
+	 */
+	task_thread_info(prev_p)->saved_preempt_count = this_cpu_read(__preempt_count);
+	this_cpu_write(__preempt_count, task_thread_info(next_p)->saved_preempt_count);
+
 	this_cpu_write(kernel_stack,
 		  (unsigned long)task_stack_page(next_p) +
 		  THREAD_SIZE - KERNEL_STACK_OFFSET);
--- a/arch/x86/kernel/setup-xen.c
+++ b/arch/x86/kernel/setup-xen.c
@@ -1115,6 +1115,7 @@ void __init setup_arch(char **cmdline_p)
 
 	if (is_initial_xendomain()) {
 		dmi_scan_machine();
+		dmi_memdev_walk();
 		dmi_set_dump_stack_arch_desc();
 	} else {
 		int ver = HYPERVISOR_xen_version(XENVER_version, NULL);
@@ -1267,8 +1268,6 @@ void __init setup_arch(char **cmdline_p)
 	acpi_initrd_override((void *)initrd_start, initrd_end - initrd_start);
 #endif
 
-	reserve_crashkernel();
-
 	vsmp_init();
 
 	io_delay_init();
@@ -1288,6 +1287,13 @@ void __init setup_arch(char **cmdline_p)
 	early_acpi_boot_init();
 
 	initmem_init();
+
+	/*
+	 * Reserve memory for crash kernel after SRAT is parsed so that it
+	 * won't consume hotpluggable memory.
+	 */
+	reserve_crashkernel();
+
 	memblock_find_dma_reserve();
 
 #ifdef CONFIG_KVM_GUEST
--- /dev/null
+++ b/arch/x86/kernel/tracepoint-xen.c
@@ -0,0 +1,53 @@
+/*
+ * Code for supporting irq vector tracepoints.
+ *
+ * Copyright (C) 2013 Seiji Aguchi <seiji.aguchi@hds.com>
+ *
+ */
+#include <asm/desc.h>
+#include <linux/atomic.h>
+
+atomic_t trace_trap_table_ctr = ATOMIC_INIT(0);
+
+static int trace_trap_table_refcount;
+static DEFINE_MUTEX(trap_table_mutex);
+
+static void set_trace_trap_table_ctr(int val)
+{
+	atomic_set(&trace_trap_table_ctr, val);
+	/* Ensure the trace_trap_table_ctr is set before sending IPI */
+	wmb();
+}
+
+static void switch_trap_table(void *arg)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	load_current_trap_table();
+	local_irq_restore(flags);
+}
+
+void trace_irq_vector_regfunc(void)
+{
+	mutex_lock(&trap_table_mutex);
+	if (!trace_trap_table_refcount) {
+		set_trace_trap_table_ctr(1);
+		smp_call_function(switch_trap_table, NULL, 0);
+		switch_trap_table(NULL);
+	}
+	trace_trap_table_refcount++;
+	mutex_unlock(&trap_table_mutex);
+}
+
+void trace_irq_vector_unregfunc(void)
+{
+	mutex_lock(&trap_table_mutex);
+	trace_trap_table_refcount--;
+	if (!trace_trap_table_refcount) {
+		set_trace_trap_table_ctr(0);
+		smp_call_function(switch_trap_table, NULL, 0);
+		switch_trap_table(NULL);
+	}
+	mutex_unlock(&trap_table_mutex);
+}
--- a/arch/x86/kernel/traps-xen.c
+++ b/arch/x86/kernel/traps-xen.c
@@ -94,7 +94,7 @@ static inline void conditional_sti(struc
 
 static inline void preempt_conditional_sti(struct pt_regs *regs)
 {
-	inc_preempt_count();
+	preempt_count_inc();
 	if (regs->flags & X86_EFLAGS_IF)
 		local_irq_enable();
 }
@@ -109,7 +109,7 @@ static inline void preempt_conditional_c
 {
 	if (regs->flags & X86_EFLAGS_IF)
 		local_irq_disable();
-	dec_preempt_count();
+	preempt_count_dec();
 }
 
 static int __kprobes
@@ -663,7 +663,7 @@ static void _math_state_restore(void)
 		return;
 	}
 
-	tsk->fpu_counter++;
+	tsk->thread.fpu_counter++;
 }
 
 void math_state_restore(void)
@@ -772,6 +772,33 @@ static const trap_info_t trap_table[] =
 	{ }
 };
 
+#ifdef CONFIG_TRACING
+static const trap_info_t trace_trap_table[] = {
+	{ X86_TRAP_DE, 0|X, __KERNEL_CS, (unsigned long)trace_divide_error	},
+	{ X86_TRAP_BR, 0|X, __KERNEL_CS, (unsigned long)trace_bounds		},
+	{ X86_TRAP_UD, 0|X, __KERNEL_CS, (unsigned long)trace_invalid_op	},
+	{ X86_TRAP_NM, 0|4, __KERNEL_CS, (unsigned long)trace_device_not_available },
+	{ X86_TRAP_OLD_MF, 0|X, __KERNEL_CS, (unsigned long)trace_coprocessor_segment_overrun },
+	{ X86_TRAP_TS, 0|X, __KERNEL_CS, (unsigned long)trace_invalid_TSS	},
+	{ X86_TRAP_NP, 0|X, __KERNEL_CS, (unsigned long)trace_segment_not_present },
+	{ X86_TRAP_GP, 0|X, __KERNEL_CS, (unsigned long)trace_general_protection },
+	{ X86_TRAP_PF, 0|4, __KERNEL_CS, (unsigned long)trace_page_fault	},
+	{ X86_TRAP_MF, 0|X, __KERNEL_CS, (unsigned long)trace_coprocessor_error	},
+	{ X86_TRAP_AC, 0|X, __KERNEL_CS, (unsigned long)trace_alignment_check	},
+	{ X86_TRAP_XF, 0|X, __KERNEL_CS, (unsigned long)trace_simd_coprocessor_error },
+	{ }
+};
+
+void load_current_trap_table(void)
+{
+	const trap_info_t *tt = is_trace_trap_table_enabled()
+				? trace_trap_table : trap_table;
+
+	if (HYPERVISOR_set_trap_table(tt))
+		BUG();
+}
+#endif
+
 /* Set of traps needed for early debugging. */
 void __init early_trap_init(void)
 {
--- a/arch/x86/mm/fault-xen.c
+++ b/arch/x86/mm/fault-xen.c
@@ -20,6 +20,9 @@
 #include <asm/kmemcheck.h>		/* kmemcheck_*(), ...		*/
 #include <asm/fixmap.h>			/* VSYSCALL_START		*/
 
+#define CREATE_TRACE_POINTS
+#include <asm/trace/exceptions.h>
+
 /*
  * Page fault error code bits:
  *
@@ -51,7 +54,7 @@ kmmio_fault(struct pt_regs *regs, unsign
 	return 0;
 }
 
-static inline int __kprobes notify_page_fault(struct pt_regs *regs)
+static inline int __kprobes kprobes_fault(struct pt_regs *regs)
 {
 	int ret = 0;
 
@@ -605,7 +608,7 @@ show_fault_oops(struct pt_regs *regs, un
 
 	printk(KERN_CONT " at %p\n", (void *) address);
 	printk(KERN_ALERT "IP:");
-	printk_address(regs->ip, 1);
+	printk_address(regs->ip);
 
 	dump_pagetable(address);
 }
@@ -647,6 +650,20 @@ no_context(struct pt_regs *regs, unsigne
 
 	/* Are we prepared to handle this kernel fault? */
 	if (fixup_exception(regs)) {
+		/*
+		 * Any interrupt that takes a fault gets the fixup. This makes
+		 * the below recursive fault logic only apply to a faults from
+		 * task context.
+		 */
+		if (in_interrupt())
+			return;
+
+		/*
+		 * Per the above we're !in_interrupt(), aka. task context.
+		 *
+		 * In this case we need to make sure we're not recursively
+		 * faulting through the emulate_vsyscall() logic.
+		 */
 		if (current_thread_info()->sig_on_uaccess_error && signal) {
 			tsk->thread.trap_nr = X86_TRAP_PF;
 			tsk->thread.error_code = error_code | PF_USER;
@@ -655,6 +672,10 @@ no_context(struct pt_regs *regs, unsigne
 			/* XXX: hwpoison faults will set the wrong code. */
 			force_sig_info_fault(signal, si_code, address, tsk, 0);
 		}
+
+		/*
+		 * Barring that, we can do the fixup and be happy.
+		 */
 		return;
 	}
 
@@ -1077,7 +1098,7 @@ __do_page_fault(struct pt_regs *regs, un
 			return;
 
 		/* kprobes don't want to hook the spurious faults: */
-		if (notify_page_fault(regs))
+		if (kprobes_fault(regs))
 			return;
 		/*
 		 * Don't take the mm semaphore here. If we fixup a prefetch
@@ -1089,23 +1110,8 @@ __do_page_fault(struct pt_regs *regs, un
 	}
 
 	/* kprobes don't want to hook the spurious faults: */
-	if (unlikely(notify_page_fault(regs)))
+	if (unlikely(kprobes_fault(regs)))
 		return;
-	/*
-	 * It's safe to allow irq's after cr2 has been saved and the
-	 * vmalloc fault has been handled.
-	 *
-	 * User-mode registers count as a user access even for any
-	 * potential system fault or CPU buglet:
-	 */
-	if (user_mode_vm(regs)) {
-		local_irq_enable();
-		error_code |= PF_USER;
-		flags |= FAULT_FLAG_USER;
-	} else {
-		if (regs->flags & X86_EFLAGS_IF)
-			local_irq_enable();
-	}
 
 	if (unlikely(error_code & PF_RSVD))
 		pgtable_bad(regs, error_code, address);
@@ -1117,8 +1123,6 @@ __do_page_fault(struct pt_regs *regs, un
 		}
 	}
 
-	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
-
 	/*
 	 * If we're in an interrupt, have no user context or are running
 	 * in an atomic region then we must not take the fault:
@@ -1128,6 +1132,24 @@ __do_page_fault(struct pt_regs *regs, un
 		return;
 	}
 
+	/*
+	 * It's safe to allow irq's after cr2 has been saved and the
+	 * vmalloc fault has been handled.
+	 *
+	 * User-mode registers count as a user access even for any
+	 * potential system fault or CPU buglet:
+	 */
+	if (user_mode_vm(regs)) {
+		local_irq_enable();
+		error_code |= PF_USER;
+		flags |= FAULT_FLAG_USER;
+	} else {
+		if (regs->flags & X86_EFLAGS_IF)
+			local_irq_enable();
+	}
+
+	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
+
 	if (error_code & PF_WRITE)
 		flags |= FAULT_FLAG_WRITE;
 
@@ -1260,3 +1282,23 @@ do_page_fault(struct pt_regs *regs, unsi
 	__do_page_fault(regs, error_code);
 	exception_exit(prev_state);
 }
+
+static void trace_page_fault_entries(struct pt_regs *regs,
+				     unsigned long error_code)
+{
+	if (user_mode(regs))
+		trace_page_fault_user(read_cr2(), regs, error_code);
+	else
+		trace_page_fault_kernel(read_cr2(), regs, error_code);
+}
+
+dotraplinkage void __kprobes
+trace_do_page_fault(struct pt_regs *regs, unsigned long error_code)
+{
+	enum ctx_state prev_state;
+
+	prev_state = exception_enter();
+	trace_page_fault_entries(regs, error_code);
+	__do_page_fault(regs, error_code);
+	exception_exit(prev_state);
+}
--- a/arch/x86/mm/init-xen.c
+++ b/arch/x86/mm/init-xen.c
@@ -53,12 +53,12 @@ __ref void *alloc_low_pages(unsigned int
 	if ((pgt_buf_end + num) > pgt_buf_top || !can_use_brk_pgt) {
 		unsigned long ret;
 		if (min_pfn_mapped >= max_pfn_mapped)
-			panic("alloc_low_page: ran out of memory");
+			panic("alloc_low_pages: ran out of memory");
 		ret = memblock_find_in_range(min_pfn_mapped << PAGE_SHIFT,
 					max_pfn_mapped << PAGE_SHIFT,
 					PAGE_SIZE * num , PAGE_SIZE);
 		if (!ret)
-			panic("alloc_low_page: can not alloc memory");
+			panic("alloc_low_pages: can not alloc memory");
 		memblock_reserve(ret, PAGE_SIZE * num);
 		pfn = ret >> PAGE_SHIFT;
 	} else {
@@ -416,26 +416,46 @@ static unsigned long __init init_range_m
 	return mapped_ram_size;
 }
 
-/* (PUD_SHIFT-PMD_SHIFT)/2 */
-#define STEP_SIZE_SHIFT 5
-void __init init_mem_mapping(void)
+static unsigned long __init get_new_step_size(unsigned long step_size)
+{
+	/*
+	 * Explain why we shift by 5 and why we don't have to worry about
+	 * 'step_size << 5' overflowing:
+	 *
+	 * initial mapped size is PMD_SIZE (2M).
+	 * We can not set step_size to be PUD_SIZE (1G) yet.
+	 * In worse case, when we cross the 1G boundary, and
+	 * PG_LEVEL_2M is not set, we will need 1+1+512 pages (2M + 8k)
+	 * to map 1G range with PTE. Use 5 as shift for now.
+	 *
+	 * Don't need to worry about overflow, on 32bit, when step_size
+	 * is 0, round_down() returns 0 for start, and that turns it
+	 * into 0x100000000ULL.
+	 */
+	return step_size << 5;
+}
+
+/**
+ * memory_map_top_down - Map [map_start, map_end) top down
+ * @map_start: start address of the target memory range
+ * @map_end: end address of the target memory range
+ *
+ * This function will setup direct mapping for memory range
+ * [map_start, map_end) in top-down. That said, the page tables
+ * will be allocated at the end of the memory, and we map the
+ * memory in top-down.
+ */
+static void __init memory_map_top_down(unsigned long map_start,
+				       unsigned long map_end)
 {
-	unsigned long end, real_end, start, last_start;
+	unsigned long real_end, start, last_start;
 	unsigned long step_size;
 	unsigned long addr;
 	unsigned long mapped_ram_size = 0;
 	unsigned long new_mapped_ram_size;
 
-	probe_page_size_mask();
-
-#ifdef CONFIG_X86_64
-	end = max_pfn << PAGE_SHIFT;
-#else
-	end = max_low_pfn << PAGE_SHIFT;
-#endif
-
 	/* xen has big range in reserved near end of ram, skip it at first.*/
-	addr = memblock_find_in_range(ISA_END_ADDRESS, end, PMD_SIZE, PMD_SIZE);
+	addr = memblock_find_in_range(map_start, map_end, PMD_SIZE, PMD_SIZE);
 	real_end = addr + PMD_SIZE;
 
 	/* step_size need to be small so pgt_buf from BRK could cover it */
@@ -450,20 +470,103 @@ void __init init_mem_mapping(void)
 	 * end of RAM in [min_pfn_mapped, max_pfn_mapped) used as new pages
 	 * for page table.
 	 */
-	while (last_start) {
-		start = round_down(last_start - 1, step_size);
+	while (last_start > map_start) {
+		if (last_start > step_size) {
+			start = round_down(last_start - 1, step_size);
+			if (start < map_start)
+				start = map_start;
+		} else
+			start = map_start;
 		new_mapped_ram_size = init_range_memory_mapping(start,
 							last_start);
 		last_start = start;
 		min_pfn_mapped = last_start >> PAGE_SHIFT;
 		/* only increase step_size after big range get mapped */
 		if (new_mapped_ram_size > mapped_ram_size)
-			step_size <<= STEP_SIZE_SHIFT;
+			step_size = get_new_step_size(step_size);
+		mapped_ram_size += new_mapped_ram_size;
+	}
+
+	if (real_end < map_end)
+		init_range_memory_mapping(real_end, map_end);
+}
+
+/**
+ * memory_map_bottom_up - Map [map_start, map_end) bottom up
+ * @map_start: start address of the target memory range
+ * @map_end: end address of the target memory range
+ *
+ * This function will setup direct mapping for memory range
+ * [map_start, map_end) in bottom-up. Since we have limited the
+ * bottom-up allocation above the kernel, the page tables will
+ * be allocated just above the kernel and we map the memory
+ * in [map_start, map_end) in bottom-up.
+ */
+static void __init memory_map_bottom_up(unsigned long map_start,
+					unsigned long map_end)
+{
+	unsigned long next, new_mapped_ram_size, start;
+	unsigned long mapped_ram_size = 0;
+	/* step_size need to be small so pgt_buf from BRK could cover it */
+	unsigned long step_size = PMD_SIZE;
+
+	start = map_start;
+	min_pfn_mapped = start >> PAGE_SHIFT;
+
+	/*
+	 * We start from the bottom (@map_start) and go to the top (@map_end).
+	 * The memblock_find_in_range() gets us a block of RAM from the
+	 * end of RAM in [min_pfn_mapped, max_pfn_mapped) used as new pages
+	 * for page table.
+	 */
+	while (start < map_end) {
+		if (map_end - start > step_size) {
+			next = round_up(start + 1, step_size);
+			if (next > map_end)
+				next = map_end;
+		} else
+			next = map_end;
+
+		new_mapped_ram_size = init_range_memory_mapping(start, next);
+		start = next;
+
+		if (new_mapped_ram_size > mapped_ram_size)
+			step_size = get_new_step_size(step_size);
 		mapped_ram_size += new_mapped_ram_size;
 	}
+}
+
+void __init init_mem_mapping(void)
+{
+	unsigned long end;
+
+	probe_page_size_mask();
+
+#ifdef CONFIG_X86_64
+	end = max_pfn << PAGE_SHIFT;
+#else
+	end = max_low_pfn << PAGE_SHIFT;
+#endif
 
-	if (real_end < end)
-		init_range_memory_mapping(real_end, end);
+	/*
+	 * If the allocation is in bottom-up direction, we setup direct mapping
+	 * in bottom-up, otherwise we setup direct mapping in top-down.
+	 */
+	if (memblock_bottom_up()) {
+		unsigned long kernel_end = __pa_symbol(_end);
+
+		/*
+		 * we need two separate calls here. This is because we want to
+		 * allocate page tables above the kernel. So we first map
+		 * [kernel_end, end) to make memory above the kernel be mapped
+		 * as soon as possible. And then use page tables allocated above
+		 * the kernel to map [0, kernel_end).
+		 */
+		memory_map_bottom_up(kernel_end, end);
+		memory_map_bottom_up(0, kernel_end);
+	} else {
+		memory_map_top_down(0, end);
+	}
 
 #ifdef CONFIG_X86_64
 	if (max_pfn > max_low_pfn) {
--- a/arch/x86/mm/pgtable-xen.c
+++ b/arch/x86/mm/pgtable-xen.c
@@ -38,11 +38,14 @@ pgtable_t pte_alloc_one(struct mm_struct
 	struct page *pte;
 
 	pte = alloc_pages(__userpte_alloc_gfp, 0);
-	if (pte) {
-		pgtable_page_ctor(pte);
-		SetPageForeign(pte, _pte_free);
-		init_page_count(pte);
+	if (!pte)
+		return NULL;
+	if (!pgtable_page_ctor(pte)) {
+		__free_page(pte);
+		return NULL;
 	}
+	SetPageForeign(pte, _pte_free);
+	init_page_count(pte);
 	return pte;
 }
 
@@ -110,6 +113,10 @@ pmd_t *pmd_alloc_one(struct mm_struct *m
 	pmd = alloc_pages(PGALLOC_GFP, 0);
 	if (!pmd)
 		return NULL;
+	if (!pgtable_pmd_page_ctor(pmd)) {
+		__free_page(pmd);
+		return NULL;
+	}
 	SetPageForeign(pmd, _pmd_free);
 	init_page_count(pmd);
 	return page_address(pmd);
@@ -128,11 +135,13 @@ void __pmd_free(pgtable_t pmd)
 
 	ClearPageForeign(pmd);
 	init_page_count(pmd);
+	pgtable_pmd_page_dtor(pmd);
 	__free_page(pmd);
 }
 
 void ___pmd_free_tlb(struct mmu_gather *tlb, pmd_t *pmd)
 {
+	struct page *page = virt_to_page(pmd);
 	paravirt_release_pmd(__pa(pmd) >> PAGE_SHIFT);
 	/*
 	 * NOTE! For PAE, any changes to the top page-directory-pointer-table
@@ -141,7 +150,8 @@ void ___pmd_free_tlb(struct mmu_gather *
 #ifdef CONFIG_X86_PAE
 	tlb->need_flush_all = 1;
 #endif
-	tlb_remove_page(tlb, virt_to_page(pmd));
+	pgtable_pmd_page_dtor(page);
+	tlb_remove_page(tlb, page);
 }
 
 #if PAGETABLE_LEVELS > 3
@@ -156,7 +166,7 @@ void ___pud_free_tlb(struct mmu_gather *
 static void _pin_lock(struct mm_struct *mm, int lock) {
 	if (lock)
 		spin_lock(&mm->page_table_lock);
-#if USE_SPLIT_PTLOCKS
+#if USE_SPLIT_PTE_PTLOCKS
 	/*
 	 * While mm->page_table_lock protects us against insertions and
 	 * removals of higher level page table pages, it doesn't protect
--- a/drivers/acpi/Kconfig
+++ b/drivers/acpi/Kconfig
@@ -369,7 +369,7 @@ source "drivers/acpi/apei/Kconfig"
 
 config ACPI_EXTLOG
 	tristate "Extended Error Log support"
-	depends on X86_MCE && X86_LOCAL_APIC
+	depends on X86_MCE && X86_LOCAL_APIC && !XEN
 	select UEFI_CPER
 	default n
 	help
--- a/drivers/acpi/acpi_processor.c
+++ b/drivers/acpi/acpi_processor.c
@@ -283,9 +283,7 @@ static int acpi_processor_get_info(struc
 	 */
 	if (pr->id == -1) {
 		int ret = acpi_processor_hotadd_init(pr);
-		if (ret && (ret != -ENODEV ||
-			    acpi_get_cpuid(pr->handle, ~device_declaration,
-					   pr->acpi_id) < 0))
+		if (ret && (ret != -ENODEV || pr->apic_id == -1))
 			return ret;
 	}
 #if defined(CONFIG_SMP) && defined(CONFIG_PROCESSOR_EXTERNAL_CONTROL)
--- a/drivers/acpi/processor_core.c
+++ b/drivers/acpi/processor_core.c
@@ -193,7 +193,7 @@ int acpi_get_apicid(acpi_handle handle,
 
 int acpi_map_cpuid(int apic_id, u32 acpi_id)
 {
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && !defined(CONFIG_PROCESSOR_EXTERNAL_CONTROL)
 	int i;
 #endif
 
--- a/drivers/hwmon/coretemp-xen.c
+++ b/drivers/hwmon/coretemp-xen.c
@@ -53,7 +53,7 @@ MODULE_PARM_DESC(tjmax, "TjMax value in
 
 #define BASE_SYSFS_ATTR_NO	2	/* Sysfs Base attr no for coretemp */
 #define NUM_REAL_CORES		32	/* Number of Real cores per cpu */
-#define CORETEMP_NAME_LENGTH	17	/* String Length of attrs */
+#define CORETEMP_NAME_LENGTH	19	/* String Length of attrs */
 #define MAX_CORE_ATTRS		4	/* Maximum no of basic attrs */
 #define TOTAL_ATTRS		(MAX_CORE_ATTRS + 1)
 #define MAX_CORE_DATA		(NUM_REAL_CORES + BASE_SYSFS_ATTR_NO)
--- a/drivers/misc/mic/Kconfig
+++ b/drivers/misc/mic/Kconfig
@@ -2,7 +2,7 @@ comment "Intel MIC Host Driver"
 
 config INTEL_MIC_HOST
 	tristate "Intel MIC Host Driver"
-	depends on 64BIT && PCI && X86
+	depends on 64BIT && PCI && X86 && !XEN
 	select VHOST_RING
 	help
 	  This enables Host Driver support for the Intel Many Integrated
@@ -22,7 +22,7 @@ comment "Intel MIC Card Driver"
 
 config INTEL_MIC_CARD
 	tristate "Intel MIC Card Driver"
-	depends on 64BIT && X86
+	depends on 64BIT && X86 && !XEN
 	select VIRTIO
 	help
 	  This enables card driver support for the Intel Many Integrated
--- a/drivers/pci/msi-xen.c
+++ b/drivers/pci/msi-xen.c
@@ -236,7 +236,7 @@ static u64 find_table_base(struct pci_de
 	unsigned long flags;
 
 	pci_read_config_dword(dev, dev->msix_cap + PCI_MSIX_TABLE, &reg);
-	bar = reg & PCI_MSIX_FLAGS_BIRMASK;
+	bar = reg & PCI_MSIX_PBA_BIR;
 
 	flags = pci_resource_flags(dev, bar);
 	if (!flags ||
@@ -642,7 +642,7 @@ static int msix_capability_init(struct p
  * @nvec: how many MSIs have been requested ?
  * @type: are we checking for MSI or MSI-X ?
  *
- * Look at global flags, the device itself, and its parent busses
+ * Look at global flags, the device itself, and its parent buses
  * to determine if MSI/-X are supported for the device. If MSI/-X is
  * supported return 0, else return an error code.
  **/
@@ -700,7 +700,7 @@ int pci_enable_msi_block(struct pci_dev
 	u16 msgctl;
 	struct msi_dev_list *msi_dev_entry = get_msi_dev_pirq_list(dev);
 
-	if (!dev->msi_cap)
+	if (!dev->msi_cap || dev->current_state != PCI_D0)
 		return -EINVAL;
 
 	pci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &msgctl);
@@ -759,7 +759,7 @@ int pci_enable_msi_block_auto(struct pci
 	int ret, nvec;
 	u16 msgctl;
 
-	if (!dev->msi_cap)
+	if (!dev->msi_cap || dev->current_state != PCI_D0)
 		return -EINVAL;
 
 	pci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &msgctl);
@@ -855,7 +855,7 @@ int pci_enable_msix(struct pci_dev *dev,
 	int i, j, temp;
 	struct msi_dev_list *msi_dev_entry = get_msi_dev_pirq_list(dev);
 
-	if (!entries || !dev->msix_cap)
+	if (!entries || !dev->msix_cap || dev->current_state != PCI_D0)
 		return -EINVAL;
 
 	if (!is_initial_xendomain()) {
--- a/drivers/powercap/Kconfig
+++ b/drivers/powercap/Kconfig
@@ -17,7 +17,7 @@ if POWERCAP
 # Client driver configurations go here.
 config INTEL_RAPL
 	tristate "Intel RAPL Support"
-	depends on X86
+	depends on X86 && !XEN
 	default n
 	---help---
 	  This enables support for the Intel Running Average Power Limit (RAPL)
--- a/drivers/xen/netback/common.h
+++ b/drivers/xen/netback/common.h
@@ -67,7 +67,9 @@ typedef struct netif_st {
 	/* Frontend feature information. */
 	u8 can_sg:1;
 	u8 gso:1;
+	u8 gso6:1;
 	u8 csum:1;
+	u8 csum6:1;
 
 	/* Internal feature information. */
 	u8 can_queue:1;	/* can queue packets for receiver? */
--- a/drivers/xen/netback/interface.c
+++ b/drivers/xen/netback/interface.c
@@ -108,8 +108,12 @@ static netdev_features_t netbk_fix_featu
 		features &= ~NETIF_F_SG;
 	if (!netif->gso)
 		features &= ~NETIF_F_TSO;
+	if (!netif->gso6)
+		features &= ~NETIF_F_TSO6;
 	if (!netif->csum)
 		features &= ~NETIF_F_IP_CSUM;
+	if (!netif->csum6)
+		features &= ~NETIF_F_IPV6_CSUM;
 
 	return features;
 }
@@ -218,7 +222,8 @@ netif_t *netif_alloc(struct device *pare
 
 	dev->netdev_ops = &netif_be_netdev_ops;
 
-	dev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO;
+	dev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM
+			   | NETIF_F_TSO | NETIF_F_TSO6;
 	dev->features = dev->hw_features;
 
 	SET_ETHTOOL_OPS(dev, &network_ethtool_ops);
--- a/drivers/xen/netback/netback.c
+++ b/drivers/xen/netback/netback.c
@@ -311,7 +311,7 @@ static struct sk_buff *netbk_copy_skb(st
 
 static inline unsigned int netbk_max_required_rx_slots(const netif_t *netif)
 {
-	return netif->can_sg || netif->gso
+	return netif->can_sg || netif->gso || netif->gso6
 	       ? max_t(unsigned int, XEN_NETIF_NR_SLOTS_MIN,
 		       MAX_SKB_FRAGS + 2/* header + extra_info + frags */)
 	       : 1; /* all in one */
@@ -840,7 +840,14 @@ static void net_rx_action(unsigned long
 			resp->flags |= XEN_NETRXF_extra_info;
 
 			gso->u.gso.size = meta[npo.meta_cons].frag.size;
-			gso->u.gso.type = XEN_NETIF_GSO_TYPE_TCPV4;
+			if (!gso->u.gso.size)
+				gso->u.gso.type = XEN_NETIF_GSO_TYPE_NONE;
+			else if (meta[npo.meta_cons].frag.page_offset & SKB_GSO_TCPV4)
+				gso->u.gso.type = XEN_NETIF_GSO_TYPE_TCPV4;
+			else if (meta[npo.meta_cons].frag.page_offset & SKB_GSO_TCPV6)
+				gso->u.gso.type = XEN_NETIF_GSO_TYPE_TCPV6;
+			else
+				gso->u.gso.type = XEN_NETIF_GSO_TYPE_NONE;
 			gso->u.gso.pad = 0;
 			gso->u.gso.features = 0;
 
@@ -1470,15 +1477,20 @@ static int netbk_set_skb_gso(netif_t *ne
 		return -EINVAL;
 	}
 
-	/* Currently only TCPv4 S.O. is supported. */
-	if (gso->u.gso.type != XEN_NETIF_GSO_TYPE_TCPV4) {
+	switch (gso->u.gso.type) {
+	case XEN_NETIF_GSO_TYPE_TCPV4:
+		skb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;
+		break;
+	case XEN_NETIF_GSO_TYPE_TCPV6:
+		skb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;
+		break;
+	default:
 		netdev_err(skb->dev, "Bad GSO type %d.\n", gso->u.gso.type);
 		netbk_fatal_tx_err(netif);
 		return -EINVAL;
 	}
 
 	skb_shinfo(skb)->gso_size = gso->u.gso.size;
-	skb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;
 
 	/* Header must be checked, and gso_segs computed. */
 	skb_shinfo(skb)->gso_type |= SKB_GSO_DODGY;
--- a/drivers/xen/netback/xenbus.c
+++ b/drivers/xen/netback/xenbus.c
@@ -107,6 +107,21 @@ static int netback_probe(struct xenbus_d
 			goto abort_transaction;
 		}
 
+		err = xenbus_printf(xbt, dev->nodename, "feature-gso-tcpv6",
+				    "%d", sg);
+		if (err) {
+			message = "writing feature-gso-tcpv6";
+			goto abort_transaction;
+		}
+
+		/* We support partial checksum setup for IPv6 packets */
+		err = xenbus_write(xbt, dev->nodename,
+				   "feature-ipv6-csum-offload", "1");
+		if (err) {
+			message = "writing feature-ipv6-csum-offload";
+			goto abort_transaction;
+		}
+
 		/* We support rx-copy path. */
 		err = xenbus_write(xbt, dev->nodename, "feature-rx-copy", "1");
 		if (err) {
@@ -448,11 +463,21 @@ static int connect_rings(struct backend_
 		val = 0;
 	netif->gso = !!val;
 
+	if (xenbus_scanf(XBT_NIL, dev->otherend, "feature-gso-tcpv6", "%d",
+			 &val) < 0)
+		val = 0;
+	netif->gso6 = !!val;
+
 	if (xenbus_scanf(XBT_NIL, dev->otherend, "feature-no-csum-offload",
 			 "%d", &val) < 0)
 		val = 0;
 	netif->csum = !val;
 
+	if (xenbus_scanf(XBT_NIL, dev->otherend, "feature-ipv6-csum-offload",
+			 "%d", &val) < 0)
+		val = 0;
+	netif->csum6 = !!val;
+
 	/* Map the shared frame, irq etc. */
 	err = netif_map(be, tx_ring_ref, rx_ring_ref, evtchn);
 	if (err) {
--- a/drivers/xen/netfront/netfront.c
+++ b/drivers/xen/netfront/netfront.c
@@ -452,6 +452,17 @@ again:
 		goto abort_transaction;
 	}
 
+#ifdef NETIF_F_IPV6_CSUM
+	err = xenbus_write(xbt, dev->nodename, "feature-ipv6-csum-offload",
+			   "1");
+	if (err) {
+		message = "writing feature-ipv6-csum-offload";
+		goto abort_transaction;
+	}
+#else
+#define NETIF_F_IPV6_CSUM 0
+#endif
+
 	err = xenbus_write(xbt, dev->nodename, "feature-sg", "1");
 	if (err) {
 		message = "writing feature-sg";
@@ -1489,7 +1500,7 @@ err:
 		u64_stats_update_end(&stats->syncp);
 
 		/* Pass it up. */
-		netif_receive_skb(skb);
+		napi_gro_receive(napi, skb);
 	}
 
 	/* If we get a callback with very few responses, reduce fill target. */
@@ -1516,6 +1527,8 @@ err:
 	}
 
 	if (work_done < budget) {
+		napi_gro_flush(napi, false);
+
 		local_irq_save(flags);
 
 		RING_FINAL_CHECK_FOR_RESPONSES(&np->rx, more_to_do);
@@ -2078,6 +2091,15 @@ static netdev_features_t xennet_fix_feat
 			features &= ~NETIF_F_TSO;
 	}
 
+	if (features & NETIF_F_IPV6_CSUM) {
+		if (xenbus_scanf(XBT_NIL, np->xbdev->otherend,
+				 "feature-ipv6-csum-offload", "%d", &val) < 0)
+			val = 0;
+
+		if (!val)
+			features &= ~NETIF_F_IPV6_CSUM;
+	}
+
 	return features;
 }
 
@@ -2147,6 +2169,8 @@ static struct net_device *create_netdev(
 	np->stats = alloc_percpu(struct netfront_stats);
 	if (np->stats == NULL)
 		goto exit;
+	for_each_possible_cpu(i)
+		u64_stats_init(&per_cpu_ptr(np->stats, i)->syncp);
 
 	/* Initialise {tx,rx}_skbs as a free chain containing every entry. */
 	for (i = 0; i <= NET_TX_RING_SIZE; i++) {
@@ -2177,7 +2201,8 @@ static struct net_device *create_netdev(
 	netdev->netdev_ops	= &xennet_netdev_ops;
 	netif_napi_add(netdev, &np->napi, netif_poll, 64);
 	netdev->features        = NETIF_F_RXCSUM | NETIF_F_GSO_ROBUST;
-	netdev->hw_features	= NETIF_F_IP_CSUM | NETIF_F_SG | NETIF_F_TSO;
+	netdev->hw_features	= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM
+				  | NETIF_F_SG | NETIF_F_TSO;
 
 	/*
          * Assume that all hw features are available for now. This set
--- a/drivers/xen/pci.c
+++ b/drivers/xen/pci.c
@@ -212,7 +212,7 @@ static int __init register_xen_pci_notif
 
 arch_initcall(register_xen_pci_notifier);
 
-#ifdef CONFIG_PCI_MMCONFIG
+#if defined(CONFIG_PCI_MMCONFIG) && !defined(CONFIG_XEN)
 static int __init xen_mcfg_late(void)
 {
 	struct pci_mmcfg_region *cfg;
--- a/drivers/xen/xenbus/xenbus_probe.c
+++ b/drivers/xen/xenbus/xenbus_probe.c
@@ -473,6 +473,9 @@ static ssize_t nodename_show(struct devi
 {
 	return sprintf(buf, "%s\n", to_xenbus_device(dev)->nodename);
 }
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,13,0)
+static DEVICE_ATTR_RO(nodename);
+#endif
 
 static ssize_t devtype_show(struct device *dev,
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,13)
@@ -482,6 +485,9 @@ static ssize_t devtype_show(struct devic
 {
 	return sprintf(buf, "%s\n", to_xenbus_device(dev)->devicetype);
 }
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,13,0)
+static DEVICE_ATTR_RO(devtype);
+#endif
 
 static ssize_t modalias_show(struct device *dev,
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,13)
@@ -492,14 +498,33 @@ static ssize_t modalias_show(struct devi
 	return sprintf(buf, "%s:%s\n", dev->bus->name,
 		       to_xenbus_device(dev)->devicetype);
 }
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,13,0)
+static DEVICE_ATTR_RO(modalias);
+
+static struct attribute *xenbus_dev_attrs[] = {
+	&dev_attr_nodename.attr,
+	&dev_attr_devtype.attr,
+	&dev_attr_modalias.attr,
+	NULL
+};
 
-struct device_attribute xenbus_dev_attrs[] = {
+static const struct attribute_group xenbus_dev_group = {
+	.attrs = xenbus_dev_attrs,
+};
+
+const struct attribute_group *xenbus_dev_groups[] = {
+	&xenbus_dev_group,
+	NULL
+};
+PARAVIRT_EXPORT_SYMBOL(xenbus_dev_groups);
+#else /* LINUX_VERSION_CODE < KERNEL_VERSION(3,13,0) */
+static struct device_attribute xenbus_dev_attrs[] = {
 	__ATTR_RO(nodename),
 	__ATTR_RO(devtype),
 	__ATTR_RO(modalias),
 	__ATTR_NULL
 };
-PARAVIRT_EXPORT_SYMBOL(xenbus_dev_attrs);
+#endif
 
 int xenbus_probe_node(struct xen_bus_type *bus,
 		      const char *type,
@@ -649,7 +674,9 @@ static struct xen_bus_type xenbus_fronte
 		.shutdown  = xenbus_dev_shutdown,
 		.uevent    = xenbus_uevent_frontend,
 #endif
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,29)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,13,0)
+		.dev_groups = xenbus_dev_groups,
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,29)
 		.dev_attrs = xenbus_dev_attrs,
 #endif
 	},
--- a/drivers/xen/xenbus/xenbus_probe.h
+++ b/drivers/xen/xenbus/xenbus_probe.h
@@ -84,7 +84,7 @@ enum xenstore_init {
 	XS_LOCAL,
 };
 
-extern struct device_attribute xenbus_dev_attrs[];
+extern const struct attribute_group *xenbus_dev_groups[];
 
 extern int xenbus_match(struct device *_dev, struct device_driver *_drv);
 extern int xenbus_dev_probe(struct device *_dev);
--- a/drivers/xen/xenbus/xenbus_probe_backend.c
+++ b/drivers/xen/xenbus/xenbus_probe_backend.c
@@ -223,7 +223,7 @@ static struct xen_bus_type xenbus_backen
 #ifdef CONFIG_XEN
 		.shutdown	= xenbus_dev_shutdown,
 #endif
-		.dev_attrs	= xenbus_dev_attrs,
+		.dev_groups	= xenbus_dev_groups,
 	},
 };
 
--- a/include/xen/interface/io/blkif.h
+++ b/include/xen/interface/io/blkif.h
@@ -544,7 +544,7 @@ struct blkif_request {
         struct __attribute__((__packed__)) blkif_request_rw {
             uint8_t        nr_segments;  /* number of segments                  */
             blkif_vdev_t   handle;       /* only for read/write requests        */
-#ifdef CONFIG_X86_64
+#ifndef CONFIG_X86_32
             uint32_t       _pad1;        /* offsetof(blkif_request,u.rw.id) == 8 */
 #endif
             uint64_t       id;           /* private guest value, echoed in resp */
@@ -555,7 +555,7 @@ struct blkif_request {
             uint8_t        flag;         /* BLKIF_DISCARD_SECURE or zero.        */
 #define BLKIF_DISCARD_SECURE (1<<0)      /* ignored if discard-secure=0          */
             blkif_vdev_t   _pad1;        /* only for read/write requests         */
-#ifdef CONFIG_X86_64
+#ifndef CONFIG_X86_32
             uint32_t       _pad2;        /* offsetof(blkif_req..,u.discard.id)==8*/
 #endif
             uint64_t       id;           /* private guest value, echoed in resp  */
@@ -566,7 +566,7 @@ struct blkif_request {
 	struct __attribute__((__packed__)) blkif_request_other {
 		uint8_t      _pad1;
 		blkif_vdev_t _pad2;      /* only for read/write requests         */
-#ifdef CONFIG_X86_64
+#ifndef CONFIG_X86_32
 		uint32_t     _pad3;      /* offsetof(blkif_req..,u.other.id)==8*/
 #endif
 		uint64_t     id;         /* private guest value, echoed in resp  */
@@ -574,7 +574,7 @@ struct blkif_request {
 	struct __attribute__((__packed__)) blkif_request_indirect {
 		uint8_t        indirect_op;
 		uint16_t       nr_segments;
-#ifdef CONFIG_X86_64
+#ifndef CONFIG_X86_32
 		uint32_t       _pad1;    /* offsetof(blkif_...,u.indirect.id) == 8 */
 #endif
 		uint64_t       id;
@@ -582,7 +582,7 @@ struct blkif_request {
 		blkif_vdev_t   handle;
 		uint16_t       _pad2;
 		grant_ref_t    indirect_grefs[BLKIF_MAX_INDIRECT_PAGES_PER_REQUEST];
-#ifdef CONFIG_X86_64
+#ifndef CONFIG_X86_32
 		uint32_t      _pad3;     /* make it 64 byte aligned */
 #else
 		uint64_t      _pad3;     /* make it 64 byte aligned */
--- a/include/xen/net-util.h
+++ b/include/xen/net-util.h
@@ -6,12 +6,191 @@
 #include <linux/tcp.h>
 #include <linux/udp.h>
 #include <net/ip.h>
+#include <net/ip6_checksum.h>
+
+static inline int _maybe_pull_tail(struct sk_buff *skb, unsigned int len,
+				   unsigned int max)
+{
+	if (skb_headlen(skb) >= len)
+		return 0;
+
+	/*
+	 * If we need to pullup then pullup to the max, so we won't need to
+	 * do it again.
+	 */
+	if (max > skb->len)
+		max = skb->len;
+
+	if (!__pskb_pull_tail(skb, max - skb_headlen(skb)))
+		return -ENOMEM;
+
+	if (skb_headlen(skb) < len)
+		return -EPROTO;
+
+	return 0;
+}
+
+static inline __be16 *_checksum_setup_ip(struct sk_buff *skb,
+					 typeof(IPPROTO_IP) proto,
+					 unsigned int off, unsigned int ver)
+{
+	switch (proto) {
+		int err;
+
+	case IPPROTO_TCP:
+		err = _maybe_pull_tail(skb, off + sizeof(struct tcphdr),
+				       off + sizeof(struct tcphdr));
+		if (!err && !skb_partial_csum_set(skb, off,
+						  offsetof(struct tcphdr,
+							   check)))
+			err = -EPROTO;
+		return err ? ERR_PTR(err) : &tcp_hdr(skb)->check;
+
+	case IPPROTO_UDP:
+		err = _maybe_pull_tail(skb, off + sizeof(struct udphdr),
+				       off + sizeof(struct udphdr));
+		if (!err && !skb_partial_csum_set(skb, off,
+						  offsetof(struct udphdr,
+							   check)))
+			err = -EPROTO;
+		return err ? ERR_PTR(err) : &udp_hdr(skb)->check;
+	}
+
+	net_err_ratelimited("Attempting to checksum a non-TCPv%u/UDPv%u packet,"
+			    " dropping a protocol %d packet\n",
+			    proto, ver, ver);
+	return ERR_PTR(-EPROTO);
+}
+
+/*
+ * This value should be large enough to cover a tagged ethernet header plus
+ * maximally sized IP and TCP or UDP headers.
+ */
+#define MAX_IP_HDR_LEN 128
+
+static inline int _checksum_setup_ipv4(struct sk_buff *skb, bool recalc)
+{
+	const struct iphdr *iph;
+	__be16 *csum;
+	unsigned int off;
+	int err = _maybe_pull_tail(skb, sizeof(*iph), MAX_IP_HDR_LEN);
+
+	if (err)
+		return err;
+	iph = ip_hdr(skb);
+	if (iph->frag_off & htons(IP_OFFSET | IP_MF)) {
+		net_err_ratelimited("Packet is a fragment\n");
+		return -EPROTO;
+	}
+	off = 4 * iph->ihl;
+	csum = _checksum_setup_ip(skb, iph->protocol, off, 4);
+	if (IS_ERR(csum))
+		return PTR_ERR(csum);
+
+	if (recalc) {
+		iph = ip_hdr(skb);
+		*csum = ~csum_tcpudp_magic(iph->saddr, iph->daddr,
+					   skb->len - off, iph->protocol, 0);
+	}
+
+	return 0;
+}
+
+/*
+ * This value should be large enough to cover a tagged ethernet header plus
+ * an IPv6 header, all options, and a maximal TCP or UDP header.
+ */
+#define MAX_IPV6_HDR_LEN 256
+
+static inline int _checksum_setup_ipv6(struct sk_buff *skb, bool recalc)
+{
+#define OPT_HDR(type, skb, off) ((type *)(skb_network_header(skb) + (off)))
+	const struct ipv6hdr *ipv6h;
+	__be16 *csum;
+	unsigned int off = sizeof(*ipv6h);
+	u8 nexthdr;
+	bool done = false, fragment = false;
+	int err = _maybe_pull_tail(skb, off, MAX_IPV6_HDR_LEN);
+
+	if (err)
+		return err;
+	ipv6h = ipv6_hdr(skb);
+	nexthdr = ipv6h->nexthdr;
+
+	while ((off <= sizeof(*ipv6h) + ntohs(ipv6h->payload_len)) && !done) {
+		switch (nexthdr) {
+		case IPPROTO_DSTOPTS:
+		case IPPROTO_HOPOPTS:
+		case IPPROTO_ROUTING: {
+			const struct ipv6_opt_hdr *hp;
+
+			err = _maybe_pull_tail(skb, off + sizeof(*hp),
+					       MAX_IPV6_HDR_LEN);
+			if (err)
+				return err;
+			hp = OPT_HDR(struct ipv6_opt_hdr, skb, off);
+			nexthdr = hp->nexthdr;
+			off += ipv6_optlen(hp);
+			break;
+		}
+		case IPPROTO_AH: {
+			const struct ip_auth_hdr *hp;
+
+			err = _maybe_pull_tail(skb, off + sizeof(*hp),
+					       MAX_IPV6_HDR_LEN);
+			if (err)
+				return err;
+			hp = OPT_HDR(struct ip_auth_hdr, skb, off);
+			nexthdr = hp->nexthdr;
+			off += ipv6_authlen(hp);
+			break;
+		}
+		case IPPROTO_FRAGMENT: {
+			const struct frag_hdr *hp;
+
+			err = _maybe_pull_tail(skb, off + sizeof(*hp),
+					       MAX_IPV6_HDR_LEN);
+			if (err < 0)
+				return err;
+			hp = OPT_HDR(struct frag_hdr, skb, off);
+			if (hp->frag_off & htons(IP6_OFFSET | IP6_MF))
+				fragment = true;
+			nexthdr = hp->nexthdr;
+			off += sizeof(*hp);
+			break;
+		}
+		default:
+			done = true;
+			break;
+		}
+		ipv6h = ipv6_hdr(skb);
+	}
+
+	if (!done || fragment) {
+		net_err_ratelimited("%s\n",
+				    done ? "Failed to parse packet header"
+					 : "Packet is a v6 fragment");
+		return -EPROTO;
+	}
+
+	csum = _checksum_setup_ip(skb, nexthdr, off, 4);
+	if (IS_ERR(csum))
+		return PTR_ERR(csum);
+
+	if (recalc) {
+		ipv6h = ipv6_hdr(skb);
+		*csum = ~csum_ipv6_magic(&ipv6h->saddr, &ipv6h->daddr,
+					 skb->len - off, nexthdr, 0);
+	}
+
+	return 0;
+#undef OPT_HDR
+}
 
 static inline int skb_checksum_setup(struct sk_buff *skb,
 				     unsigned long *fixup_counter)
 {
-	struct iphdr *iph = (void *)skb->data;
-	__be16 *csum = NULL;
+	bool recalc = false;
 	int err = -EPROTO;
 
 	skb_reset_network_header(skb);
@@ -27,43 +206,20 @@ static inline int skb_checksum_setup(str
 		 * recalculate the partial checksum.
 		 */
 		++*fixup_counter;
-		--csum;
+		recalc = true;
 	}
 
-	if (skb->protocol != htons(ETH_P_IP))
-		goto out;
-
-	switch (iph->protocol) {
-	case IPPROTO_TCP:
-		if (!skb_partial_csum_set(skb, 4 * iph->ihl,
-					  offsetof(struct tcphdr, check)))
-			goto out;
-		if (csum)
-			csum = &tcp_hdr(skb)->check;
+	switch (skb->protocol) {
+	case htons(ETH_P_IP):
+		err = _checksum_setup_ipv4(skb, recalc);
 		break;
-	case IPPROTO_UDP:
-		if (!skb_partial_csum_set(skb, 4 * iph->ihl,
-					  offsetof(struct udphdr, check)))
-			goto out;
-		if (csum)
-			csum = &udp_hdr(skb)->check;
+	case htons(ETH_P_IPV6):
+		err = _checksum_setup_ipv6(skb, recalc);
 		break;
-	default:
-		net_err_ratelimited("Attempting to checksum a non-TCP/UDP packet,"
-				    " dropping a protocol %d packet\n",
-				    iph->protocol);
-		goto out;
 	}
+	if (!err)
+		skb_probe_transport_header(skb, 0);
 
-	if (csum)
-		*csum = ~csum_tcpudp_magic(iph->saddr, iph->daddr,
-					   skb->len - iph->ihl*4,
-					   iph->protocol, 0);
-
-	skb_probe_transport_header(skb, 0);
-
-	err = 0;
-out:
 	return err;
 }
 
--- a/lib/swiotlb-xen.c
+++ b/lib/swiotlb-xen.c
@@ -35,6 +35,9 @@
 #include <xen/interface/memory.h>
 #include <asm/gnttab_dma.h>
 
+#define CREATE_TRACE_POINTS
+#include <trace/events/swiotlb.h>
+
 #define OFFSET(val,align) ((unsigned long)((val) & ( (align) - 1)))
 
 int swiotlb;
@@ -445,6 +448,7 @@ phys_addr_t swiotlb_tbl_map_single(struc
 
 not_found:
 	spin_unlock_irqrestore(&io_tlb_lock, flags);
+	dev_warn(hwdev, "swiotlb buffer is full\n");
 	return SWIOTLB_MAP_ERROR;
 found:
 	spin_unlock_irqrestore(&io_tlb_lock, flags);
@@ -603,6 +607,8 @@ dma_addr_t swiotlb_map_page(struct devic
 	    !range_needs_mapping(phys, size))
 		return dev_addr;
 
+	trace_swiotlb_bounced(dev, dev_addr, size, 1);
+
 	/* Oh well, have to allocate and map a bounce buffer. */
 	gnttab_dma_unmap_page(dev_addr);
 	map = map_single(dev, phys, size, dir);
